{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d74b62",
   "metadata": {},
   "source": [
    "# Q3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ae7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the project root directory\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805526e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_configs(project_root, config_filename):\n",
    "    with open(os.path.join(project_root, \"config\", config_filename), 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def _build_transforms(img_size=224):\n",
    "    train_tfms = T.Compose([\n",
    "        T.RandomResizedCrop(img_size, scale=(0.6, 1.0), ratio=(0.75, 1.33)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "        T.RandomErasing(p=0.1, scale=(0.02, 0.08)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tfms = T.Compose([\n",
    "        T.Resize(int(img_size * 1.15)),\n",
    "        T.CenterCrop(img_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tfms, val_tfms\n",
    "\n",
    "def get_train_val_data_loaders(data_root, img_size, batch_size, val_ratio=0.2, seed=42):\n",
    "    train_tfms, val_tfms = _build_transforms(img_size)\n",
    "\n",
    "    full_ds = ImageFolder(root=os.path.join(data_root, \"train\"),\n",
    "                          transform=train_tfms)\n",
    "    labels  = full_ds.targets\n",
    "    indices = list(range(len(full_ds)))\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, test_size=val_ratio, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    train_ds = torch.utils.data.Subset(full_ds, train_idx)\n",
    "    val_ds   = torch.utils.data.Subset(full_ds, val_idx)\n",
    "    # override the transform on the val subset\n",
    "    val_ds.dataset.transform = val_tfms  # deterministic pipeline for val\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, full_ds.classes\n",
    "\n",
    "\n",
    "def get_test_data_loader(data_root, img_size, batch_size):\n",
    "    _, val_tfms = _build_transforms(img_size)\n",
    "    test_ds = ImageFolder(root=os.path.join(data_root, \"test\"),\n",
    "                          transform=val_tfms)\n",
    "    return DataLoader(test_ds, batch_size=batch_size,\n",
    "                      shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_resnet_on_test(model, test_loader, device):\n",
    "    # compute test accuracy + confusion data\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds  = logits.argmax(dim=1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    test_acc = sum(int(t==p) for t,p in zip(y_true, y_pred)) / len(y_true)\n",
    "    return y_true, y_pred, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d772db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 10 classes\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PartB_Q3_Solution_Pretrained</strong> at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy/runs/61t2j956' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy/runs/61t2j956</a><br> View project at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250419_221038-61t2j956/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/wandb/run-20250419_221130-vivv6bkc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy/runs/vivv6bkc' target=\"_blank\">PartB_Q3_Solution_Pretrained</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy/runs/vivv6bkc' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/dummy/runs/vivv6bkc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: ResNet (10 classes)\n",
      "Epoch: 01/20]  Train_Loss: 1.164 Train_Accuracy: 0.752 | Val_Loss: 0.848 Val_Accuracy: 0.871\n",
      "  >> New best val‑acc 0.871 (checkpoint saved)\n",
      "Epoch: 02/20]  Train_Loss: 0.695 Train_Accuracy: 0.941 | Val_Loss: 0.788 Val_Accuracy: 0.897\n",
      "  >> New best val‑acc 0.897 (checkpoint saved)\n",
      "Epoch: 03/20]  Train_Loss: 0.585 Train_Accuracy: 0.985 | Val_Loss: 0.771 Val_Accuracy: 0.905\n",
      "  >> New best val‑acc 0.905 (checkpoint saved)\n",
      ">> Unfreezing layer3 at epoch 4\n",
      "Epoch: 04/20]  Train_Loss: 0.552 Train_Accuracy: 0.994 | Val_Loss: 0.765 Val_Accuracy: 0.907\n",
      "  >> New best val‑acc 0.907 (checkpoint saved)\n",
      "Epoch: 05/20]  Train_Loss: 0.533 Train_Accuracy: 0.999 | Val_Loss: 0.761 Val_Accuracy: 0.900\n",
      "  >> No improvement (1/3)\n",
      "Epoch: 06/20]  Train_Loss: 0.528 Train_Accuracy: 0.999 | Val_Loss: 0.769 Val_Accuracy: 0.900\n",
      "  >> No improvement (2/3)\n",
      "Epoch: 07/20]  Train_Loss: 0.525 Train_Accuracy: 1.000 | Val_Loss: 0.754 Val_Accuracy: 0.909\n",
      "  >> New best val‑acc 0.909 (checkpoint saved)\n",
      "Epoch: 08/20]  Train_Loss: 0.522 Train_Accuracy: 1.000 | Val_Loss: 0.767 Val_Accuracy: 0.903\n",
      "  >> No improvement (1/3)\n",
      "Epoch: 09/20]  Train_Loss: 0.518 Train_Accuracy: 1.000 | Val_Loss: 0.752 Val_Accuracy: 0.907\n",
      "  >> No improvement (2/3)\n",
      "Epoch: 10/20]  Train_Loss: 0.514 Train_Accuracy: 1.000 | Val_Loss: 0.755 Val_Accuracy: 0.908\n",
      "  >> No improvement (3/3)\n",
      "Early stopping.\n",
      "Training done. Best val‑acc = 0.909\n",
      "Loading best model for final evaluation...\n",
      "Getting test data loader...\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2901858/3490615390.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 90.90%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"   # select GPU\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) MODEL BUILDING (freeze/unfreeze)\n",
    "# ----------------------------------------------------------------\n",
    "def build_finetune_resnet50(num_classes, freeze_until_layer=3):\n",
    "    model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "    # freeze all\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    # unfreeze from layer4 upwards\n",
    "    layer_names = [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "    for ln in layer_names[freeze_until_layer:]:\n",
    "        for p in getattr(model, ln).parameters():\n",
    "            p.requires_grad = True\n",
    "    # replace classifier\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) MAIN TRAIN LOOP\n",
    "# ------------------------------------------------------------------\n",
    "def main(static_config):\n",
    "    model_config   = static_config[\"model_config\"]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Data loading\n",
    "    set_seeds(model_config[\"seed\"])\n",
    "    train_dl, val_dl, class_names = get_train_val_data_loaders(\n",
    "        data_root = os.path.join(project_root, static_config[\"data_root\"]),\n",
    "        img_size  = model_config[\"resize_dim\"],\n",
    "        batch_size= model_config[\"batch_size\"],\n",
    "        val_ratio = model_config[\"val_ratio\"],\n",
    "        seed      = model_config[\"seed\"]\n",
    "    )\n",
    "    num_classes = len(class_names)\n",
    "    print(f\"Loaded data: {num_classes} classes\")\n",
    "\n",
    "    # W&B initialization\n",
    "    wandb.init(\n",
    "        project   = static_config[\"wandb_project\"],\n",
    "        name      = static_config[\"wandb_run_name\"],\n",
    "        config    = {**model_config, \"num_classes\": num_classes}\n",
    "    )\n",
    "    wandb.watch_called = False\n",
    "    # watch model once it's on device:\n",
    "    model = build_finetune_resnet50(num_classes,\n",
    "                                    freeze_until_layer=model_config[\"freeze_until_layer\"])\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    print(f\"Model loaded: {model.__class__.__name__} ({num_classes} classes)\")\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "\n",
    "    # --- optimizer with discriminative LRs for head & layer4 only ---\n",
    "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    head_params   = list(base_model.fc.parameters())\n",
    "    layer4_params = list(base_model.layer4.parameters())\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": head_params,   \"lr\": 1e-3},\n",
    "        {\"params\": layer4_params, \"lr\": 1e-4},\n",
    "    ], weight_decay=1e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    patience      = model_config[\"patience\"]\n",
    "    best_val_acc  = -float(\"inf\")\n",
    "    patience_cnt  = 0\n",
    "    # os.makedirs(project_root, static_config[\"output_dir\"], exist_ok=True)\n",
    "    ckpt_path     = os.path.join(project_root, static_config[\"output_dir\"], \"dummy_partB_Q3_best_resnet50.pth\")\n",
    "\n",
    "    UNFREEZE_EPOCH = 4\n",
    "    epochs         = model_config[\"epochs\"]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # staged unfreeze of layer3\n",
    "        if epoch == UNFREEZE_EPOCH:\n",
    "            print(f\">> Unfreezing layer3 at epoch {epoch}\")\n",
    "            layer3 = base_model.layer3\n",
    "            for p in layer3.parameters():\n",
    "                p.requires_grad = True\n",
    "            optimizer.add_param_group({\n",
    "                \"params\": list(layer3.parameters()),\n",
    "                \"lr\": 1e-4\n",
    "            })\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_dl, optimizer, criterion, device)\n",
    "        val_loss,   val_acc   = validate_one_epoch(model, val_dl,   criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch:02d}/{epochs}]  Train_Loss: {train_loss:.3f} Train_Accuracy: {train_acc:.3f} | \"\n",
    "              f\"Val_Loss: {val_loss:.3f} Val_Accuracy: {val_acc:.3f}\")\n",
    "        \n",
    "        #  log to W&B\n",
    "        wandb.log({\n",
    "            \"PartB_Q3_epoch\":         epoch,\n",
    "            \"PartB_Q3_train_loss\":    train_loss,\n",
    "            \"PartB_Q3_train_acc\":     train_acc,\n",
    "            \"PartB_Q3_val_loss\":      val_loss,\n",
    "            \"PartB_Q3_val_acc\":       val_acc,\n",
    "            \"PartB_Q3_lr_head\":       optimizer.param_groups[0][\"lr\"],\n",
    "            \"PartB_Q3_lr_layer4\":     optimizer.param_groups[1][\"lr\"],\n",
    "        })\n",
    "\n",
    "        # early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_cnt = 0\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"  >> New best val‑acc {best_val_acc:.3f} (checkpoint saved)\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            print(f\"  >> No improvement ({patience_cnt}/{patience})\")\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # load best & final test eval\n",
    "    print(f\"Training done. Best val‑acc = {best_val_acc:.3f}\")\n",
    "\n",
    "    print(\"Loading best model for final evaluation...\")\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    print(\"Getting test data loader...\")\n",
    "    test_loader = get_test_data_loader(os.path.join(project_root, static_config[\"data_root\"]), model_config[\"resize_dim\"], model_config[\"batch_size\"])\n",
    "\n",
    "    print(\"Evaluating on test set...\")\n",
    "    y_true, y_pred, test_acc = evaluate_resnet_on_test(model, test_loader, device)\n",
    "    print(f\"Test accuracy = {test_acc*100:.2f}%\")\n",
    "    \n",
    "    # log final metrics and plot\n",
    "    wandb.log({\"PartB_Q3_test_acc\": test_acc})\n",
    "    cm = wandb.plot.confusion_matrix(\n",
    "        probs    = None,\n",
    "        y_true   = y_true,\n",
    "        preds    = y_pred,\n",
    "        class_names = class_names\n",
    "    )\n",
    "    wandb.log({\"PartB_Q3_confusion_matrix\": cm})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = get_configs(project_root, \"configs.yaml\")[\"part_b_configs\"][\"solution_3_configs\"]\n",
    "    main(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_jax_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
