{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569571c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-14 11:17:07--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.195.155, 142.250.207.91, 142.250.182.123, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.195.155|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3816687935 (3.6G) [application/zip]\n",
      "Saving to: ‘nature_12K.zip’\n",
      "\n",
      "nature_12K.zip      100%[===================>]   3.55G  10.8MB/s    in 5m 36s  \n",
      "\n",
      "2025-04-14 11:22:43 (10.8 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ../inaturalist_data/ https://storage.googleapis.com/wandb_datasets/nature_12K.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57082dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: inaturalist_12K/\n",
      "File: inaturalist_12K/.DS_Store\n",
      "Directory: inaturalist_12K/train/\n",
      "Directory: inaturalist_12K/train/Plantae/\n",
      "File: inaturalist_12K/train/Plantae/1dfc3916ad58af6ce9af9fc8b36ceedd.jpg\n",
      "File: inaturalist_12K/train/Plantae/e49eeff2b27ae74351fdf8ffa3791307.jpg\n",
      "File: inaturalist_12K/train/Plantae/519ae1c558dff337bb4084934e31a4a8.jpg\n",
      "File: inaturalist_12K/train/Plantae/2232663628cd9c456a6d01c90ba961c4.jpg\n",
      "File: inaturalist_12K/train/Plantae/e26227424586b97ab3377a567ac4f006.jpg\n",
      "Directory: inaturalist_12K/train/Aves/\n",
      "File: inaturalist_12K/train/Aves/d55072249be7621868a3e62cae31ac29.jpg\n",
      "File: inaturalist_12K/train/Aves/e53ee219fded8973f5295c2c628b3391.jpg\n",
      "File: inaturalist_12K/train/Aves/2d0243d0dc0c6132b4d982c6617fed69.jpg\n",
      "File: inaturalist_12K/train/Aves/4fe826e09bc9e1de1d5ced2e12c9e3b8.jpg\n",
      "File: inaturalist_12K/train/Aves/b92c670548d19740c14a9add63a4277b.jpg\n",
      "Directory: inaturalist_12K/train/Amphibia/\n",
      "File: inaturalist_12K/train/Amphibia/3a7954b5e4efa5ef52aeba3d792adcc0.jpg\n",
      "File: inaturalist_12K/train/Amphibia/ef12baa265e2bbccbb45966849e532a9.jpg\n",
      "File: inaturalist_12K/train/Amphibia/20fe73a29d1791da708c516d9811c4f5.jpg\n",
      "File: inaturalist_12K/train/Amphibia/342f6ee2768f88a00c7cbb718b9b4b95.jpg\n",
      "File: inaturalist_12K/train/Amphibia/68e8d96c35b873056950ceed3d302792.jpg\n",
      "Directory: inaturalist_12K/train/Insecta/\n",
      "File: inaturalist_12K/train/Insecta/aa0ecf3512e425b3e41264957f9d06bb.jpg\n",
      "File: inaturalist_12K/train/Insecta/944fe9dfef3f869ea4bafbe0be6ce6bc.jpg\n",
      "File: inaturalist_12K/train/Insecta/c43b8a90d132e79bc89092c0f0c901cf.jpg\n",
      "File: inaturalist_12K/train/Insecta/bf212987e1feb82bf447d76afc167b9a.jpg\n",
      "File: inaturalist_12K/train/Insecta/7b3c37066b9c9bea3520d5e3fc486dc7.jpg\n",
      "Directory: inaturalist_12K/train/Animalia/\n",
      "File: inaturalist_12K/train/Animalia/539d062b531af72de6bb6a2a280cd612.jpg\n",
      "File: inaturalist_12K/train/Animalia/def893c32da14fa2f5e03fe7e0f83b6f.jpg\n",
      "File: inaturalist_12K/train/Animalia/a3ad64116c71f9e10f660eeb6b738ee3.jpg\n",
      "File: inaturalist_12K/train/Animalia/78dceaefee9e5c1235ee6d0b93f6461f.jpg\n",
      "File: inaturalist_12K/train/Animalia/aaad1c76ac3a21a7c1a23bdac3efd9ad.jpg\n",
      "Directory: inaturalist_12K/train/Mollusca/\n",
      "File: inaturalist_12K/train/Mollusca/fbefac79a828ce0853704146aff0a155.jpg\n",
      "File: inaturalist_12K/train/Mollusca/ab27b9dccd0499897cb4cb15fdc97d9f.jpg\n",
      "File: inaturalist_12K/train/Mollusca/28f9784990121b7259a0e11db85621eb.jpg\n",
      "File: inaturalist_12K/train/Mollusca/04593370b1498f5daa20bd0c5905d35f.jpg\n",
      "File: inaturalist_12K/train/Mollusca/b2d95329e81114207d5b5f3c5e332c1f.jpg\n",
      "Directory: inaturalist_12K/train/Fungi/\n",
      "File: inaturalist_12K/train/Fungi/c71c1bfc79031765b626be48a62ec96c.jpg\n",
      "File: inaturalist_12K/train/Fungi/f1f39b2b307def7bcb34a7b58d29cc14.jpg\n",
      "File: inaturalist_12K/train/Fungi/dc57ca12143eb493bb67c295330d5b6e.jpg\n",
      "File: inaturalist_12K/train/Fungi/6caa9430110003ec6a1d8b9ec363ebc2.jpg\n",
      "File: inaturalist_12K/train/Fungi/b9bea1e69cc1ee087ffcc2ce521bf46d.jpg\n",
      "Directory: inaturalist_12K/train/Arachnida/\n",
      "File: inaturalist_12K/train/Arachnida/ba10899d4db1aaeb836fcc9b2bbd8099.jpg\n",
      "File: inaturalist_12K/train/Arachnida/30f39d28fbe145c6c336262286d3a8f9.jpg\n",
      "File: inaturalist_12K/train/Arachnida/ae5e80d726e8209fc8fc42698f08efbd.jpg\n",
      "File: inaturalist_12K/train/Arachnida/b13497d3a338d8a04c95c88b1f09c8aa.jpg\n",
      "File: inaturalist_12K/train/Arachnida/f3b8c7dd76d9b492c12487a6cb29c561.jpg\n",
      "Directory: inaturalist_12K/train/Reptilia/\n",
      "File: inaturalist_12K/train/Reptilia/7d101163e71cc9a9875d0811a6b8981d.jpg\n",
      "File: inaturalist_12K/train/Reptilia/567316c4cb9e5c244466a1bfd9cb30ec.jpg\n",
      "File: inaturalist_12K/train/Reptilia/438f4ad1d2acf0d04a173c5434e6c6b5.jpg\n",
      "File: inaturalist_12K/train/Reptilia/c70e3fa6762839afceb1339af149fd23.jpg\n",
      "File: inaturalist_12K/train/Reptilia/d5a827306412b894b31f8b4f36fa4a71.jpg\n",
      "Directory: inaturalist_12K/train/Mammalia/\n",
      "File: inaturalist_12K/train/Mammalia/20a284d0a80d2f3df5dedbb58d1cb1f2.jpg\n",
      "File: inaturalist_12K/train/Mammalia/dd54c074e3624f39c1748575d427f215.jpg\n",
      "File: inaturalist_12K/train/Mammalia/ba9cb0f15cac1e6f1652096a7434b228.jpg\n",
      "File: inaturalist_12K/train/Mammalia/edcddceea80af2e6d79cde996b54a3af.jpg\n",
      "File: inaturalist_12K/train/Mammalia/d90141ee4f669ba9ac9d8778cb2d9853.jpg\n",
      "Directory: inaturalist_12K/val/\n",
      "Directory: inaturalist_12K/val/Plantae/\n",
      "File: inaturalist_12K/val/Plantae/9c3137ae0e0cdc7c00763f086dd70c59.jpg\n",
      "File: inaturalist_12K/val/Plantae/8f8534b064ca568830b127f384be2369.jpg\n",
      "File: inaturalist_12K/val/Plantae/81c7385963754450403e579ff492a90e.jpg\n",
      "File: inaturalist_12K/val/Plantae/4c2a5e2a23ad5818dfbd11a0a30f4c79.jpg\n",
      "File: inaturalist_12K/val/Plantae/abc6afe48680fa12bef6fc6777ebc67e.jpg\n",
      "Directory: inaturalist_12K/val/Aves/\n",
      "File: inaturalist_12K/val/Aves/2c762dcfbbdc4efe8c49c058cdacd066.jpg\n",
      "File: inaturalist_12K/val/Aves/4b7a898a6d00f0fb23baccdf16375aae.jpg\n",
      "File: inaturalist_12K/val/Aves/625a9089bf53c0fb5e943d4f06c3ba29.jpg\n",
      "File: inaturalist_12K/val/Aves/1eab9a47872a7d24d396628a93c56ffe.jpg\n",
      "File: inaturalist_12K/val/Aves/0f40c64a49e4f742fd6cc76771b0e0ec.jpg\n",
      "Directory: inaturalist_12K/val/Amphibia/\n",
      "File: inaturalist_12K/val/Amphibia/028d5d40a43894b7ae216ec8516e2dc7.jpg\n",
      "File: inaturalist_12K/val/Amphibia/148c7ee0f255160cbac8b45ed174d790.jpg\n",
      "File: inaturalist_12K/val/Amphibia/885c8ec6ff249bda65575a515aa9b8b4.jpg\n",
      "File: inaturalist_12K/val/Amphibia/0bb839061395890c690b3bc70bd07326.jpg\n",
      "File: inaturalist_12K/val/Amphibia/5a523daac3aebb91923a3222495ee316.jpg\n",
      "Directory: inaturalist_12K/val/Insecta/\n",
      "File: inaturalist_12K/val/Insecta/b3228968497adf6969b10b62e91e93f9.jpg\n",
      "File: inaturalist_12K/val/Insecta/9b9cfe4cef82559332466d1f8ff998b5.jpg\n",
      "File: inaturalist_12K/val/Insecta/5684f03edb17c2fc72601cb278f46efb.jpg\n",
      "File: inaturalist_12K/val/Insecta/860bb68cbd43b280b79c765522d640e1.jpg\n",
      "File: inaturalist_12K/val/Insecta/1bdf9fe10658ccd1b628b63996a9c75c.jpg\n",
      "Directory: inaturalist_12K/val/Animalia/\n",
      "File: inaturalist_12K/val/Animalia/a0867a6ff6b69c24e4f19994e8d44ea3.jpg\n",
      "File: inaturalist_12K/val/Animalia/67a305a6e4b3c138edf8f1db8d9003dc.jpg\n",
      "File: inaturalist_12K/val/Animalia/9d0bd99d0c471739e0324ec91fc727f9.jpg\n",
      "File: inaturalist_12K/val/Animalia/de4f6c6c80c5d1e57dec19b50d1bbde9.jpg\n",
      "File: inaturalist_12K/val/Animalia/9e1ed7993b4c634f42f9cd7c1e478f1b.jpg\n",
      "Directory: inaturalist_12K/val/Mollusca/\n",
      "File: inaturalist_12K/val/Mollusca/50dbb613c58f154f56fbba6c3502b9ee.jpg\n",
      "File: inaturalist_12K/val/Mollusca/db7cecaccb4dac98e804678d26aee95e.jpg\n",
      "File: inaturalist_12K/val/Mollusca/caa2188f52d077e2202371af7778f989.jpg\n",
      "File: inaturalist_12K/val/Mollusca/9a55b9f4e8448b1bb9ad4c8294c65bde.jpg\n",
      "File: inaturalist_12K/val/Mollusca/e51cf7705e8e68d806744c5b18dc5aa7.jpg\n",
      "Directory: inaturalist_12K/val/Fungi/\n",
      "File: inaturalist_12K/val/Fungi/5efe26774059bd56845e98432ecd16f0.jpg\n",
      "File: inaturalist_12K/val/Fungi/a58a459c60099882ce807c3d671a4c4e.jpg\n",
      "File: inaturalist_12K/val/Fungi/0e844d9b407e06da32cf0a7272fe4a91.jpg\n",
      "File: inaturalist_12K/val/Fungi/b2e4be0623066308235ad89a6d0bfadd.jpg\n",
      "File: inaturalist_12K/val/Fungi/fa810753e621a39a9bbbed51ebd22e54.jpg\n",
      "Directory: inaturalist_12K/val/Arachnida/\n",
      "File: inaturalist_12K/val/Arachnida/cfd05e3abbbd6f514d77badc89d238e9.jpg\n",
      "File: inaturalist_12K/val/Arachnida/7d6a6d367523630f80ec8bf3370deff4.jpg\n",
      "File: inaturalist_12K/val/Arachnida/8df75120f71014b6a2763c4366849600.jpg\n",
      "File: inaturalist_12K/val/Arachnida/441445da32be8b2a97526fec9fc8ec9d.jpg\n",
      "File: inaturalist_12K/val/Arachnida/ffb1f5a773c5a644154e2b86c1f6084a.jpg\n",
      "Directory: inaturalist_12K/val/Reptilia/\n",
      "File: inaturalist_12K/val/Reptilia/d1f3615f5eb997aa0d5df1ae01087b55.jpg\n",
      "File: inaturalist_12K/val/Reptilia/c79a6ea43b44ffe55415bbd109e0e413.jpg\n",
      "File: inaturalist_12K/val/Reptilia/4f01920e16cd4b762007ca6f558e436a.jpg\n",
      "File: inaturalist_12K/val/Reptilia/775254135f5d5a5dec7216bc3953b6e7.jpg\n",
      "File: inaturalist_12K/val/Reptilia/c23724cea37df5c5cf97382094e17878.jpg\n",
      "Directory: inaturalist_12K/val/Mammalia/\n",
      "File: inaturalist_12K/val/Mammalia/f049f93b29ce13ecb99d85963d3ce751.jpg\n",
      "File: inaturalist_12K/val/Mammalia/945e0706e3398f2faa94cbefe24e9fbd.jpg\n",
      "File: inaturalist_12K/val/Mammalia/0a96565f17f08c3d82b67eed0404d189.jpg\n",
      "File: inaturalist_12K/val/Mammalia/660a84922edd76bc64924c53e26122e3.jpg\n",
      "File: inaturalist_12K/val/Mammalia/6e6e8da2ca5115e3d4d65e4f645f5043.jpg\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "def print_zip_structure(zip_file_path):\n",
    "    \"\"\"\n",
    "    Prints the directory structure of a ZIP file, including all directories and subdirectories.\n",
    "    Prints only the first 5 files, but all directories.\n",
    "    \n",
    "    Args:\n",
    "        zip_file_path (str): Path to the ZIP file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            file_count = 0  # Counter for files\n",
    "            for item in zip_file.namelist():\n",
    "                # Check if the item is a directory\n",
    "                if item.endswith('/'):\n",
    "                    file_count = 0  # Reset file count for each directory\n",
    "                    print(f\"Directory: {item}\")\n",
    "                else:\n",
    "                    # Print only the first 5 files\n",
    "                    if file_count < 5:\n",
    "                        print(f\"File: {item}\")\n",
    "                        file_count += 1\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The file is not a valid ZIP archive.\")\n",
    "\n",
    "# Example usage\n",
    "print_zip_structure(\"../inaturalist_data/nature_12K.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c189b",
   "metadata": {},
   "source": [
    "# Q1 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb7e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A modular CNN architecture:\n",
    "      - 5 x (Conv -> Activation -> MaxPool)\n",
    "      - 1 Dense (fully connected) layer of n neurons\n",
    "      - 1 Output layer of 10 neurons\n",
    "\n",
    "    Accepts images of size (3 x H x W).\n",
    "    After 5 max-pool operations (each halves H and W),\n",
    "    final feature map size is (m x (H/32) x (W/32)) if H and W are multiples of 32.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,     # m\n",
    "                 kernel_size=3,      # k\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,  # n\n",
    "                 image_height=224,   # default\n",
    "                 image_width=224     # default\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param in_channels:   Number of input channels (3 for RGB images)\n",
    "        :param num_filters:   m = number of filters in each Conv layer\n",
    "        :param kernel_size:   k = kernel size of each Conv filter (k x k)\n",
    "        :param activation_fn: Pytorch activation class, e.g., nn.ReLU\n",
    "        :param dense_neurons: n = number of neurons in the fully connected layer\n",
    "        :param image_height:  The height of the input image (assumed multiple of 32)\n",
    "        :param image_width:   The width of the input image (assumed multiple of 32)\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        # We assume 'same' padding, i.e., output of conv has same spatial size\n",
    "        # Then each MaxPool(2x2) halves the H and W each time.\n",
    "        padding = kernel_size // 2\n",
    "        Act = activation_fn  # for readability\n",
    "\n",
    "        #-------------------------\n",
    "        # 1) Block 1\n",
    "        #   Conv(in_channels->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act1 = Act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 2) Block 2\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act2 = Act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 3) Block 3\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act3 = Act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 4) Block 4\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act4 = Act()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 5) Block 5\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act5 = Act()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #--------------------------------------\n",
    "        # Compute final feature map dimension.\n",
    "        # Each pool halves H and W => H/32, W/32\n",
    "        #--------------------------------------\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        self.flatten_dim = num_filters * reduced_height * reduced_width\n",
    "\n",
    "        # Dense layer\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, dense_neurons)\n",
    "        self.act_fc1 = Act()\n",
    "\n",
    "        # Output layer: 10 neurons\n",
    "        self.output = nn.Linear(dense_neurons, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 3, H, W)\n",
    "\n",
    "        # Block 1\n",
    "        x = self.conv1(x)    # (batch_size, m, H, W)\n",
    "        x = self.act1(x)     # (batch_size, m, H, W)\n",
    "        x = self.pool1(x)    # (batch_size, m, H/2, W/2)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.conv2(x)    # (batch_size, m, H/2, W/2)\n",
    "        x = self.act2(x)     # (batch_size, m, H/2, W/2)\n",
    "        x = self.pool2(x)    # (batch_size, m, H/4, W/4)\n",
    "\n",
    "        # Block 3\n",
    "        x = self.conv3(x)    # (batch_size, m, H/4, W/4)\n",
    "        x = self.act3(x)     # (batch_size, m, H/4, W/4)\n",
    "        x = self.pool3(x)    # (batch_size, m, H/8, W/8)\n",
    "\n",
    "        # Block 4\n",
    "        x = self.conv4(x)    # (batch_size, m, H/8, W/8)\n",
    "        x = self.act4(x)     # (batch_size, m, H/8, W/8)\n",
    "        x = self.pool4(x)    # (batch_size, m, H/16, W/16)\n",
    "\n",
    "        # Block 5\n",
    "        x = self.conv5(x)    # (batch_size, m, H/16, W/16)\n",
    "        x = self.act5(x)     # (batch_size, m, H/16, W/16)\n",
    "        x = self.pool5(x)    # (batch_size, m, H/32, W/32)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, m * (H/32) * (W/32))\n",
    "\n",
    "        # Dense\n",
    "        x = self.fc1(x)            # (batch_size, dense_neurons)\n",
    "        x = self.act_fc1(x)        # (batch_size, dense_neurons)\n",
    "\n",
    "        # Output: 10 classes\n",
    "        x = self.output(x)         # (batch_size, 10)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../inaturalist_data/nature_12K_extracted already exists. Skipping extraction.\n",
      "Resizing image to resize_dim[0] x resize_dim[1] ...\n",
      "Original image size: (800, 534) (Width x Height)\n",
      "Transformed image shape: torch.Size([3, 480, 480]) (C, H, W)\n",
      "Final input shape to the model: torch.Size([1, 3, 480, 480]) (Batch, C, H, W)\n",
      "\n",
      "Model output shape: torch.Size([1, 10]) (Batch, 10)\n",
      "Raw output logits: tensor([[-0.0470,  0.0263, -0.0161,  0.0578,  0.0626, -0.0428, -0.0777, -0.0319,\n",
      "         -0.0444,  0.0573]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def extract_data_if_needed(zip_path, extract_dir):\n",
    "    \"\"\"\n",
    "    Extracts the zip file into 'extract_dir' if that folder does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        print(f\"Extracting {zip_path} to {extract_dir} ...\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=extract_dir)\n",
    "\n",
    "        print(\"Extraction done.\")\n",
    "    else:\n",
    "        print(f\"Directory {extract_dir} already exists. Skipping extraction.\")\n",
    "\n",
    "def load_single_image(image_path, resize=True, resize_dim=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads a single image with PIL, applies transforms, and returns the tensor.\n",
    "    Also prints shape info for clarity.\n",
    "    :param image_path: Path to a single .jpg file\n",
    "    :return: A PyTorch tensor of shape (1, 3, 224, 224)\n",
    "    \"\"\"\n",
    "    # Only convert to Tensor; no resizing to 224x224\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()  # # from [0..255] PIL image to [0..1] float tensor, shape => (C, H, W), range [0,1]\n",
    "    ])\n",
    "\n",
    "    # If resize is True, add resizing to the transform\n",
    "    if resize:\n",
    "        print(\"Resizing image to resize_dim[0] x resize_dim[1] ...\")\n",
    "        # Basic transform: resize to resize_dim[0] x resize_dim[1], then convert to tensor\n",
    "        transform = T.Compose([\n",
    "            T.Resize(resize_dim),  # (H, W)\n",
    "            T.ToTensor()           # from [0..255] PIL image to [0..1] float tensor, shape (C,H,W)\n",
    "        ])\n",
    "\n",
    "    # Load the image\n",
    "    pil_img = Image.open(image_path).convert('RGB')\n",
    "    print(f\"Original image size: {pil_img.size} (Width x Height)\")\n",
    "\n",
    "    # Apply transforms\n",
    "    img_tensor = transform(pil_img)  # shape: (3, H, W)\n",
    "    print(f\"Transformed image shape: {img_tensor.shape} (C, H, W)\")\n",
    "\n",
    "    # Add a batch dimension => (1, 3, H, W) i.e. (batch_size, C, H, W)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    print(f\"Final input shape to the model: {img_tensor.shape} (Batch, C, H, W)\\n\")\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "def test_model_with_image(model, image_tensor):\n",
    "    \"\"\"\n",
    "    Passes a single image tensor through the model and prints output shape.\n",
    "    :param model: Instance of MyCNN\n",
    "    :param image_tensor: shape (1, 3, H, W)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    print(f\"Model output shape: {output.shape} (Batch, 10)\")\n",
    "    print(\"Raw output logits:\", output)\n",
    "\n",
    "def main():\n",
    "    # Paths (adjust if necessary)\n",
    "    DATA_ZIP_PATH = \"../inaturalist_data/nature_12K.zip\"\n",
    "    EXTRACT_DIR   = \"../inaturalist_data/nature_12K_extracted\"\n",
    "    \n",
    "    # 1) Optional: Extract ZIP if needed\n",
    "    extract_data_if_needed(DATA_ZIP_PATH, EXTRACT_DIR)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # The ZIP has top-level folder 'inaturalist_12K' inside it.\n",
    "    # After extraction, we get:\n",
    "    #   ../inaturalist_data/inaturalist_12K_extracted/\n",
    "    #       inaturalist_12K/\n",
    "    #           train/\n",
    "    #           val/\n",
    "    #           ...\n",
    "    #\n",
    "    # So the image path must include \"inaturalist_12K\".\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # 2) Pick a single image path from the extracted data\n",
    "    #    For example, one file from 'train/Plantae' \n",
    "    sample_image_path = os.path.join(\n",
    "        EXTRACT_DIR,\n",
    "        \"inaturalist_12K\",    # top-level folder from the zip\n",
    "        \"train\",\n",
    "        \"Insecta\",\n",
    "        \"0a4a6a25d2b409ed0755097ed21fdf5b.jpg\"\n",
    "    )\n",
    "    if not os.path.isfile(sample_image_path):\n",
    "        raise FileNotFoundError(f\"Sample image not found at {sample_image_path}\")\n",
    "\n",
    "    # 3) Load and transform the image\n",
    "    image_tensor = load_single_image(sample_image_path, resize=True, resize_dim=(32*15, 32*15))\n",
    "\n",
    "    # Inspect the shape to pick your image_height, image_width\n",
    "    # For example, if the printed shape is [3, 480, 640], do:\n",
    "    _, c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "    # 4) Create model instance\n",
    "    #    Example: 16 filters each conv, kernel_size=3, dense of 128\n",
    "    model = MyCNN(in_channels=3,\n",
    "                  num_filters=16,\n",
    "                  kernel_size=3,\n",
    "                  activation_fn=nn.ReLU,\n",
    "                  dense_neurons=128,\n",
    "                  image_height=h,\n",
    "                  image_width=w)\n",
    "    print(f\"Model created with input shape: (3, {h}, {w})\")\n",
    "\n",
    "    # 5) Test forward pass\n",
    "    test_model_with_image(model, image_tensor)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10708c0b",
   "metadata": {},
   "source": [
    "# Q2 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe60877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-rain-1</strong> at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/a31dtn65' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/a31dtn65</a><br> View project at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_140617-a31dtn65/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/wandb/run-20250414_140727-o25rsf9v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/o25rsf9v' target=\"_blank\">absurd-mountain-2</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/o25rsf9v' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/o25rsf9v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-mountain-2</strong> at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/o25rsf9v' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/o25rsf9v</a><br> View project at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_140727-o25rsf9v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/wandb/run-20250414_140730-df5lucvw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/df5lucvw' target=\"_blank\">silvery-snowflake-3</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/df5lucvw' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/df5lucvw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "<class 'wandb.sdk.wandb_config.Config'> object has no attribute 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/wandb/sdk/wandb_config.py:165\u001b[0m, in \u001b[0;36mConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/wandb/sdk/wandb_config.py:130\u001b[0m, in \u001b[0;36mConfig.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'activation'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 375\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../config/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    373\u001b[0m         config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[0;32m--> 375\u001b[0m     \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq2_configs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Usage: python train_sweep.py --data_root ../inaturalist_data/nature_12K_extracted/inaturalist_12K\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 284\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    281\u001b[0m set_seeds(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msweep_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    285\u001b[0m     act_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sweep_config\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/wandb/sdk/wandb_config.py:167\u001b[0m, in \u001b[0;36mConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mke\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: <class 'wandb.sdk.wandb_config.Config'> object has no attribute 'activation'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import math\n",
    "import yaml\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Our CNN model from Q1, extended with optional BN, dropout, filter org\n",
    "# -------------------------------------------------------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,       # base number of filters\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,\n",
    "                 image_height=224,\n",
    "                 image_width=224,\n",
    "                 filter_organization=\"same\",\n",
    "                 batch_norm=False,\n",
    "                 dropout_rate=0.0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param filter_organization: \"same\", \"double_each_layer\", \"halve_each_layer\" etc.\n",
    "        :param batch_norm: if True, add nn.BatchNorm2d after each conv\n",
    "        :param dropout_rate: if > 0, we add nn.Dropout(...) in the final dense layers\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        Act = activation_fn  # convenience\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Decide how many filters each conv layer has\n",
    "        # Example logic:  \"same\" => all layers have the same # (num_filters).\n",
    "        #                 \"double_each_layer\" => [m, 2m, 4m, 8m, 16m]\n",
    "        #                 \"halve_each_layer\" => [m, m/2, m/4, m/8, m/16] etc\n",
    "        filter_sizes = []\n",
    "        if filter_organization == \"same\":\n",
    "            filter_sizes = [num_filters]*5\n",
    "        elif filter_organization == \"double_each_layer\":\n",
    "            filter_sizes = [num_filters*(2**i) for i in range(5)]\n",
    "        elif filter_organization == \"halve_each_layer\":\n",
    "            # integer cast for safety\n",
    "            filter_sizes = [max(1, num_filters//(2**i)) for i in range(5)]\n",
    "        else:\n",
    "            # fallback: same\n",
    "            filter_sizes = [num_filters]*5\n",
    "\n",
    "        # We'll store each conv block in a list\n",
    "        conv_layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in filter_sizes:\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_ch))\n",
    "            block.append(Act())\n",
    "            block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_layers.append(nn.Sequential(*block))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # After 5 max pools => image height/width are /32\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        final_ch = filter_sizes[-1]\n",
    "        self.flatten_dim = final_ch * reduced_height * reduced_width\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers_dense = []\n",
    "        layers_dense.append(nn.Linear(self.flatten_dim, dense_neurons))\n",
    "        if dropout_rate > 0.0:\n",
    "            layers_dense.append(nn.Dropout(dropout_rate))\n",
    "        layers_dense.append(Act())\n",
    "        layers_dense.append(nn.Linear(dense_neurons, 10))  # 10 output classes\n",
    "\n",
    "        self.fc = nn.Sequential(*layers_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through conv blocks\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Dense\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) Utility: Set random seeds for reproducibility\n",
    "# --------------------------------------------------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) Create a train/val split from iNaturalist\n",
    "# --------------------------------------------------------\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def load_inaturalist_train_val(data_dir,\n",
    "                               val_ratio=0.2,\n",
    "                               seed=42,\n",
    "                               augment=False,\n",
    "                               resize_dim=224):\n",
    "    \"\"\"\n",
    "    Loads the iNaturalist training data from 'data_dir' (where each subfolder \n",
    "    is a class), then splits it into train and val subsets with stratification.\n",
    "\n",
    "    :param data_dir:      Path to the folder containing subfolders of images,\n",
    "                          e.g. \".../inaturalist_12K_extracted/inaturalist_12K/train\"\n",
    "    :param val_ratio:     Fraction of data to reserve for validation (default 0.2)\n",
    "    :param seed:          Random seed to ensure reproducible splits\n",
    "    :param augment:       If True, apply creative data augmentations\n",
    "    :param resize_dim:    The final resize dimension for height & width\n",
    "                          (ideally a multiple of 32 for this CNN)\n",
    "    :return:              (train_dataset, val_dataset, class_names)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1) Define Transformations\n",
    "    # ----------------------------------------------------\n",
    "    # If you set augment=True, we'll apply some \"creative\" transformations.\n",
    "    # Otherwise, we just do a simple resize + ToTensor().\n",
    "    if augment:\n",
    "        transform_list = [\n",
    "            #  (A) Random resizing and cropping\n",
    "            T.RandomResizedCrop(size=resize_dim),\n",
    "            \n",
    "            #  (B) Random flips\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            \n",
    "            #  (C) Some random rotation\n",
    "            T.RandomRotation(degrees=30),\n",
    "            \n",
    "            #  (D) Color jitter (brightness/contrast/saturation/hue)\n",
    "            T.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2,\n",
    "                hue=0.1\n",
    "            ),\n",
    "            \n",
    "            #  (E) Small chance to invert the colors\n",
    "            T.RandomInvert(p=0.1),\n",
    "            \n",
    "            #  (F) Random perspective distortion\n",
    "            T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "            \n",
    "            #  (G) Finally, convert to Tensor\n",
    "            T.ToTensor(),\n",
    "            \n",
    "            #  (H) Optionally, random erase part of the image\n",
    "            T.RandomErasing(p=0.1)\n",
    "        ]\n",
    "    else:\n",
    "        # Minimal transform\n",
    "        transform_list = [\n",
    "            T.Resize((resize_dim, resize_dim)),\n",
    "            T.ToTensor()\n",
    "        ]\n",
    "\n",
    "    transform = T.Compose(transform_list)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2) Create the full dataset\n",
    "    # ----------------------------------------------------\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "    class_names = full_dataset.classes  # e.g. ['Amphibia', 'Animalia', ...]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3) Stratified Split\n",
    "    # ----------------------------------------------------\n",
    "    # We'll split the dataset so each class is represented proportionally \n",
    "    # in train and val. We do this manually because ImageFolder doesn't \n",
    "    # have direct stratified splitting out of the box.\n",
    "\n",
    "    # Gather indices by class\n",
    "    num_classes = len(class_names)\n",
    "    class_indices = [[] for _ in range(num_classes)]\n",
    "    \n",
    "    # Fix the random seed for reproducibility\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    for idx, (_, label) in enumerate(full_dataset):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices   = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        indices_c = class_indices[c]\n",
    "        n_c = len(indices_c)\n",
    "        n_val = int(val_ratio * n_c)\n",
    "        # Shuffle indices for this class\n",
    "        random.shuffle(indices_c)\n",
    "        val_indices_c = indices_c[:n_val]\n",
    "        train_indices_c = indices_c[n_val:]\n",
    "        val_indices.extend(val_indices_c)\n",
    "        train_indices.extend(train_indices_c)\n",
    "\n",
    "    # Build subset datasets\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "    return train_dataset, val_dataset, class_names\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4) Training loop\n",
    "# --------------------------------------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5) Main train function that W&B calls per sweep run\n",
    "# --------------------------------------------------------\n",
    "def train_and_evaluate(config):\n",
    "    # Initialize a W&B run\n",
    "    wandb.init(project=\"inat_sweep_demo\", config={\n",
    "        \"epochs\": 5\n",
    "    })\n",
    "    sweep_config = wandb.config  # short alias\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    set_seeds(42)\n",
    "\n",
    "    # Build the model\n",
    "    if sweep_config.activation == \"relu\":\n",
    "        act_fn = nn.ReLU\n",
    "    elif sweep_config.activation == \"gelu\":\n",
    "        act_fn = nn.GELU\n",
    "    elif sweep_config.activation == \"silu\":\n",
    "        act_fn = nn.SiLU\n",
    "    elif sweep_config.activation == \"mish\":\n",
    "        act_fn = nn.Mish\n",
    "    else:\n",
    "        act_fn = nn.ReLU  # fallback\n",
    "\n",
    "    model = MyCNN(\n",
    "        num_filters=sweep_config.num_filters,\n",
    "        kernel_size=3,\n",
    "        activation_fn=act_fn,\n",
    "        dense_neurons=128,\n",
    "        image_height=config['resize_dim'],  # or whatever your images are\n",
    "        image_width=config['resize_dim'],\n",
    "        filter_organization=sweep_config.filter_organization,\n",
    "        batch_norm=sweep_config.batch_norm,\n",
    "        dropout_rate=sweep_config.dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Load data\n",
    "    train_dir = os.path.join(config['data_root'], \"train\")\n",
    "    # We'll do a random 80/20 split, but keep it consistent for the sweep\n",
    "    train_dataset, val_dataset, class_names = load_inaturalist_train_val(\n",
    "        data_dir=train_dir,\n",
    "        val_ratio=0.2,\n",
    "        seed=42,\n",
    "        augment=sweep_config.data_augmentation,\n",
    "        resize_dim=config['resize_dim']\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=sweep_config.batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=sweep_config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=sweep_config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(sweep_config.epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        # Log to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{sweep_config.epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f} | \"\n",
    "              f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "\n",
    "    # Mark run finished\n",
    "    wandb.finish()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 6) Entry point if launching script manually\n",
    "#    For sweeps, W&B will call train_and_evaluate() via agent\n",
    "# --------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to do a single run without sweeps, for debugging:\n",
    "    config = None\n",
    "    with open(\"../config/config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # For a single run, we can do:\n",
    "    wandb.init(project=\"inat_sweep_demo\")\n",
    "    wandb.config.update({\n",
    "        \"num_filters\": 32,\n",
    "        \"activation\": \"relu\",\n",
    "        \"filter_organization\": \"same\",\n",
    "        \"data_augmentation\": True,\n",
    "        \"batch_norm\": True,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 2,\n",
    "        \"data_root\": config['q2_configs']['data_root']\n",
    "    })\n",
    "    \n",
    "    config = None\n",
    "    with open(\"../config/config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    train_and_evaluate(config['q2_configs'])\n",
    "\n",
    "# Usage: python train_sweep.py --data_root ../inaturalist_data/nature_12K_extracted/inaturalist_12K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df5c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_jax_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
