{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569571c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-14 11:17:07--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.195.155, 142.250.207.91, 142.250.182.123, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.195.155|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3816687935 (3.6G) [application/zip]\n",
      "Saving to: ‘nature_12K.zip’\n",
      "\n",
      "nature_12K.zip      100%[===================>]   3.55G  10.8MB/s    in 5m 36s  \n",
      "\n",
      "2025-04-14 11:22:43 (10.8 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ../inaturalist_data/ https://storage.googleapis.com/wandb_datasets/nature_12K.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57082dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: inaturalist_12K/\n",
      "File: inaturalist_12K/.DS_Store\n",
      "Directory: inaturalist_12K/train/\n",
      "Directory: inaturalist_12K/train/Plantae/\n",
      "File: inaturalist_12K/train/Plantae/1dfc3916ad58af6ce9af9fc8b36ceedd.jpg\n",
      "File: inaturalist_12K/train/Plantae/e49eeff2b27ae74351fdf8ffa3791307.jpg\n",
      "File: inaturalist_12K/train/Plantae/519ae1c558dff337bb4084934e31a4a8.jpg\n",
      "File: inaturalist_12K/train/Plantae/2232663628cd9c456a6d01c90ba961c4.jpg\n",
      "File: inaturalist_12K/train/Plantae/e26227424586b97ab3377a567ac4f006.jpg\n",
      "Directory: inaturalist_12K/train/Aves/\n",
      "File: inaturalist_12K/train/Aves/d55072249be7621868a3e62cae31ac29.jpg\n",
      "File: inaturalist_12K/train/Aves/e53ee219fded8973f5295c2c628b3391.jpg\n",
      "File: inaturalist_12K/train/Aves/2d0243d0dc0c6132b4d982c6617fed69.jpg\n",
      "File: inaturalist_12K/train/Aves/4fe826e09bc9e1de1d5ced2e12c9e3b8.jpg\n",
      "File: inaturalist_12K/train/Aves/b92c670548d19740c14a9add63a4277b.jpg\n",
      "Directory: inaturalist_12K/train/Amphibia/\n",
      "File: inaturalist_12K/train/Amphibia/3a7954b5e4efa5ef52aeba3d792adcc0.jpg\n",
      "File: inaturalist_12K/train/Amphibia/ef12baa265e2bbccbb45966849e532a9.jpg\n",
      "File: inaturalist_12K/train/Amphibia/20fe73a29d1791da708c516d9811c4f5.jpg\n",
      "File: inaturalist_12K/train/Amphibia/342f6ee2768f88a00c7cbb718b9b4b95.jpg\n",
      "File: inaturalist_12K/train/Amphibia/68e8d96c35b873056950ceed3d302792.jpg\n",
      "Directory: inaturalist_12K/train/Insecta/\n",
      "File: inaturalist_12K/train/Insecta/aa0ecf3512e425b3e41264957f9d06bb.jpg\n",
      "File: inaturalist_12K/train/Insecta/944fe9dfef3f869ea4bafbe0be6ce6bc.jpg\n",
      "File: inaturalist_12K/train/Insecta/c43b8a90d132e79bc89092c0f0c901cf.jpg\n",
      "File: inaturalist_12K/train/Insecta/bf212987e1feb82bf447d76afc167b9a.jpg\n",
      "File: inaturalist_12K/train/Insecta/7b3c37066b9c9bea3520d5e3fc486dc7.jpg\n",
      "Directory: inaturalist_12K/train/Animalia/\n",
      "File: inaturalist_12K/train/Animalia/539d062b531af72de6bb6a2a280cd612.jpg\n",
      "File: inaturalist_12K/train/Animalia/def893c32da14fa2f5e03fe7e0f83b6f.jpg\n",
      "File: inaturalist_12K/train/Animalia/a3ad64116c71f9e10f660eeb6b738ee3.jpg\n",
      "File: inaturalist_12K/train/Animalia/78dceaefee9e5c1235ee6d0b93f6461f.jpg\n",
      "File: inaturalist_12K/train/Animalia/aaad1c76ac3a21a7c1a23bdac3efd9ad.jpg\n",
      "Directory: inaturalist_12K/train/Mollusca/\n",
      "File: inaturalist_12K/train/Mollusca/fbefac79a828ce0853704146aff0a155.jpg\n",
      "File: inaturalist_12K/train/Mollusca/ab27b9dccd0499897cb4cb15fdc97d9f.jpg\n",
      "File: inaturalist_12K/train/Mollusca/28f9784990121b7259a0e11db85621eb.jpg\n",
      "File: inaturalist_12K/train/Mollusca/04593370b1498f5daa20bd0c5905d35f.jpg\n",
      "File: inaturalist_12K/train/Mollusca/b2d95329e81114207d5b5f3c5e332c1f.jpg\n",
      "Directory: inaturalist_12K/train/Fungi/\n",
      "File: inaturalist_12K/train/Fungi/c71c1bfc79031765b626be48a62ec96c.jpg\n",
      "File: inaturalist_12K/train/Fungi/f1f39b2b307def7bcb34a7b58d29cc14.jpg\n",
      "File: inaturalist_12K/train/Fungi/dc57ca12143eb493bb67c295330d5b6e.jpg\n",
      "File: inaturalist_12K/train/Fungi/6caa9430110003ec6a1d8b9ec363ebc2.jpg\n",
      "File: inaturalist_12K/train/Fungi/b9bea1e69cc1ee087ffcc2ce521bf46d.jpg\n",
      "Directory: inaturalist_12K/train/Arachnida/\n",
      "File: inaturalist_12K/train/Arachnida/ba10899d4db1aaeb836fcc9b2bbd8099.jpg\n",
      "File: inaturalist_12K/train/Arachnida/30f39d28fbe145c6c336262286d3a8f9.jpg\n",
      "File: inaturalist_12K/train/Arachnida/ae5e80d726e8209fc8fc42698f08efbd.jpg\n",
      "File: inaturalist_12K/train/Arachnida/b13497d3a338d8a04c95c88b1f09c8aa.jpg\n",
      "File: inaturalist_12K/train/Arachnida/f3b8c7dd76d9b492c12487a6cb29c561.jpg\n",
      "Directory: inaturalist_12K/train/Reptilia/\n",
      "File: inaturalist_12K/train/Reptilia/7d101163e71cc9a9875d0811a6b8981d.jpg\n",
      "File: inaturalist_12K/train/Reptilia/567316c4cb9e5c244466a1bfd9cb30ec.jpg\n",
      "File: inaturalist_12K/train/Reptilia/438f4ad1d2acf0d04a173c5434e6c6b5.jpg\n",
      "File: inaturalist_12K/train/Reptilia/c70e3fa6762839afceb1339af149fd23.jpg\n",
      "File: inaturalist_12K/train/Reptilia/d5a827306412b894b31f8b4f36fa4a71.jpg\n",
      "Directory: inaturalist_12K/train/Mammalia/\n",
      "File: inaturalist_12K/train/Mammalia/20a284d0a80d2f3df5dedbb58d1cb1f2.jpg\n",
      "File: inaturalist_12K/train/Mammalia/dd54c074e3624f39c1748575d427f215.jpg\n",
      "File: inaturalist_12K/train/Mammalia/ba9cb0f15cac1e6f1652096a7434b228.jpg\n",
      "File: inaturalist_12K/train/Mammalia/edcddceea80af2e6d79cde996b54a3af.jpg\n",
      "File: inaturalist_12K/train/Mammalia/d90141ee4f669ba9ac9d8778cb2d9853.jpg\n",
      "Directory: inaturalist_12K/val/\n",
      "Directory: inaturalist_12K/val/Plantae/\n",
      "File: inaturalist_12K/val/Plantae/9c3137ae0e0cdc7c00763f086dd70c59.jpg\n",
      "File: inaturalist_12K/val/Plantae/8f8534b064ca568830b127f384be2369.jpg\n",
      "File: inaturalist_12K/val/Plantae/81c7385963754450403e579ff492a90e.jpg\n",
      "File: inaturalist_12K/val/Plantae/4c2a5e2a23ad5818dfbd11a0a30f4c79.jpg\n",
      "File: inaturalist_12K/val/Plantae/abc6afe48680fa12bef6fc6777ebc67e.jpg\n",
      "Directory: inaturalist_12K/val/Aves/\n",
      "File: inaturalist_12K/val/Aves/2c762dcfbbdc4efe8c49c058cdacd066.jpg\n",
      "File: inaturalist_12K/val/Aves/4b7a898a6d00f0fb23baccdf16375aae.jpg\n",
      "File: inaturalist_12K/val/Aves/625a9089bf53c0fb5e943d4f06c3ba29.jpg\n",
      "File: inaturalist_12K/val/Aves/1eab9a47872a7d24d396628a93c56ffe.jpg\n",
      "File: inaturalist_12K/val/Aves/0f40c64a49e4f742fd6cc76771b0e0ec.jpg\n",
      "Directory: inaturalist_12K/val/Amphibia/\n",
      "File: inaturalist_12K/val/Amphibia/028d5d40a43894b7ae216ec8516e2dc7.jpg\n",
      "File: inaturalist_12K/val/Amphibia/148c7ee0f255160cbac8b45ed174d790.jpg\n",
      "File: inaturalist_12K/val/Amphibia/885c8ec6ff249bda65575a515aa9b8b4.jpg\n",
      "File: inaturalist_12K/val/Amphibia/0bb839061395890c690b3bc70bd07326.jpg\n",
      "File: inaturalist_12K/val/Amphibia/5a523daac3aebb91923a3222495ee316.jpg\n",
      "Directory: inaturalist_12K/val/Insecta/\n",
      "File: inaturalist_12K/val/Insecta/b3228968497adf6969b10b62e91e93f9.jpg\n",
      "File: inaturalist_12K/val/Insecta/9b9cfe4cef82559332466d1f8ff998b5.jpg\n",
      "File: inaturalist_12K/val/Insecta/5684f03edb17c2fc72601cb278f46efb.jpg\n",
      "File: inaturalist_12K/val/Insecta/860bb68cbd43b280b79c765522d640e1.jpg\n",
      "File: inaturalist_12K/val/Insecta/1bdf9fe10658ccd1b628b63996a9c75c.jpg\n",
      "Directory: inaturalist_12K/val/Animalia/\n",
      "File: inaturalist_12K/val/Animalia/a0867a6ff6b69c24e4f19994e8d44ea3.jpg\n",
      "File: inaturalist_12K/val/Animalia/67a305a6e4b3c138edf8f1db8d9003dc.jpg\n",
      "File: inaturalist_12K/val/Animalia/9d0bd99d0c471739e0324ec91fc727f9.jpg\n",
      "File: inaturalist_12K/val/Animalia/de4f6c6c80c5d1e57dec19b50d1bbde9.jpg\n",
      "File: inaturalist_12K/val/Animalia/9e1ed7993b4c634f42f9cd7c1e478f1b.jpg\n",
      "Directory: inaturalist_12K/val/Mollusca/\n",
      "File: inaturalist_12K/val/Mollusca/50dbb613c58f154f56fbba6c3502b9ee.jpg\n",
      "File: inaturalist_12K/val/Mollusca/db7cecaccb4dac98e804678d26aee95e.jpg\n",
      "File: inaturalist_12K/val/Mollusca/caa2188f52d077e2202371af7778f989.jpg\n",
      "File: inaturalist_12K/val/Mollusca/9a55b9f4e8448b1bb9ad4c8294c65bde.jpg\n",
      "File: inaturalist_12K/val/Mollusca/e51cf7705e8e68d806744c5b18dc5aa7.jpg\n",
      "Directory: inaturalist_12K/val/Fungi/\n",
      "File: inaturalist_12K/val/Fungi/5efe26774059bd56845e98432ecd16f0.jpg\n",
      "File: inaturalist_12K/val/Fungi/a58a459c60099882ce807c3d671a4c4e.jpg\n",
      "File: inaturalist_12K/val/Fungi/0e844d9b407e06da32cf0a7272fe4a91.jpg\n",
      "File: inaturalist_12K/val/Fungi/b2e4be0623066308235ad89a6d0bfadd.jpg\n",
      "File: inaturalist_12K/val/Fungi/fa810753e621a39a9bbbed51ebd22e54.jpg\n",
      "Directory: inaturalist_12K/val/Arachnida/\n",
      "File: inaturalist_12K/val/Arachnida/cfd05e3abbbd6f514d77badc89d238e9.jpg\n",
      "File: inaturalist_12K/val/Arachnida/7d6a6d367523630f80ec8bf3370deff4.jpg\n",
      "File: inaturalist_12K/val/Arachnida/8df75120f71014b6a2763c4366849600.jpg\n",
      "File: inaturalist_12K/val/Arachnida/441445da32be8b2a97526fec9fc8ec9d.jpg\n",
      "File: inaturalist_12K/val/Arachnida/ffb1f5a773c5a644154e2b86c1f6084a.jpg\n",
      "Directory: inaturalist_12K/val/Reptilia/\n",
      "File: inaturalist_12K/val/Reptilia/d1f3615f5eb997aa0d5df1ae01087b55.jpg\n",
      "File: inaturalist_12K/val/Reptilia/c79a6ea43b44ffe55415bbd109e0e413.jpg\n",
      "File: inaturalist_12K/val/Reptilia/4f01920e16cd4b762007ca6f558e436a.jpg\n",
      "File: inaturalist_12K/val/Reptilia/775254135f5d5a5dec7216bc3953b6e7.jpg\n",
      "File: inaturalist_12K/val/Reptilia/c23724cea37df5c5cf97382094e17878.jpg\n",
      "Directory: inaturalist_12K/val/Mammalia/\n",
      "File: inaturalist_12K/val/Mammalia/f049f93b29ce13ecb99d85963d3ce751.jpg\n",
      "File: inaturalist_12K/val/Mammalia/945e0706e3398f2faa94cbefe24e9fbd.jpg\n",
      "File: inaturalist_12K/val/Mammalia/0a96565f17f08c3d82b67eed0404d189.jpg\n",
      "File: inaturalist_12K/val/Mammalia/660a84922edd76bc64924c53e26122e3.jpg\n",
      "File: inaturalist_12K/val/Mammalia/6e6e8da2ca5115e3d4d65e4f645f5043.jpg\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "def print_zip_structure(zip_file_path):\n",
    "    \"\"\"\n",
    "    Prints the directory structure of a ZIP file, including all directories and subdirectories.\n",
    "    Prints only the first 5 files, but all directories.\n",
    "    \n",
    "    Args:\n",
    "        zip_file_path (str): Path to the ZIP file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            file_count = 0  # Counter for files\n",
    "            for item in zip_file.namelist():\n",
    "                # Check if the item is a directory\n",
    "                if item.endswith('/'):\n",
    "                    file_count = 0  # Reset file count for each directory\n",
    "                    print(f\"Directory: {item}\")\n",
    "                else:\n",
    "                    # Print only the first 5 files\n",
    "                    if file_count < 5:\n",
    "                        print(f\"File: {item}\")\n",
    "                        file_count += 1\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The file is not a valid ZIP archive.\")\n",
    "\n",
    "# Example usage\n",
    "print_zip_structure(\"../inaturalist_data/nature_12K.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c189b",
   "metadata": {},
   "source": [
    "# Q1 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb7e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A modular CNN architecture:\n",
    "      - 5 x (Conv -> Activation -> MaxPool)\n",
    "      - 1 Dense (fully connected) layer of n neurons\n",
    "      - 1 Output layer of 10 neurons\n",
    "\n",
    "    Accepts images of size (3 x H x W).\n",
    "    After 5 max-pool operations (each halves H and W),\n",
    "    final feature map size is (m x (H/32) x (W/32)) if H and W are multiples of 32.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,     # m\n",
    "                 kernel_size=3,      # k\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,  # n\n",
    "                 image_height=224,   # default\n",
    "                 image_width=224     # default\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param in_channels:   Number of input channels (3 for RGB images)\n",
    "        :param num_filters:   m = number of filters in each Conv layer\n",
    "        :param kernel_size:   k = kernel size of each Conv filter (k x k)\n",
    "        :param activation_fn: Pytorch activation class, e.g., nn.ReLU\n",
    "        :param dense_neurons: n = number of neurons in the fully connected layer\n",
    "        :param image_height:  The height of the input image (assumed multiple of 32)\n",
    "        :param image_width:   The width of the input image (assumed multiple of 32)\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        # We assume 'same' padding, i.e., output of conv has same spatial size\n",
    "        # Then each MaxPool(2x2) halves the H and W each time.\n",
    "        padding = kernel_size // 2\n",
    "        Act = activation_fn  # for readability\n",
    "\n",
    "        #-------------------------\n",
    "        # 1) Block 1\n",
    "        #   Conv(in_channels->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act1 = Act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 2) Block 2\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act2 = Act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 3) Block 3\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act3 = Act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 4) Block 4\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act4 = Act()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #-------------------------\n",
    "        # 5) Block 5\n",
    "        #   Conv(m->m), Act, MaxPool(2x2)\n",
    "        #-------------------------\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=num_filters,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.act5 = Act()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #--------------------------------------\n",
    "        # Compute final feature map dimension.\n",
    "        # Each pool halves H and W => H/32, W/32\n",
    "        #--------------------------------------\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        self.flatten_dim = num_filters * reduced_height * reduced_width\n",
    "\n",
    "        # Dense layer\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, dense_neurons)\n",
    "        self.act_fc1 = Act()\n",
    "\n",
    "        # Output layer: 10 neurons\n",
    "        self.output = nn.Linear(dense_neurons, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 3, H, W)\n",
    "\n",
    "        # Block 1\n",
    "        x = self.conv1(x)    # (batch_size, m, H, W)\n",
    "        x = self.act1(x)     # (batch_size, m, H, W)\n",
    "        x = self.pool1(x)    # (batch_size, m, H/2, W/2)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.conv2(x)    # (batch_size, m, H/2, W/2)\n",
    "        x = self.act2(x)     # (batch_size, m, H/2, W/2)\n",
    "        x = self.pool2(x)    # (batch_size, m, H/4, W/4)\n",
    "\n",
    "        # Block 3\n",
    "        x = self.conv3(x)    # (batch_size, m, H/4, W/4)\n",
    "        x = self.act3(x)     # (batch_size, m, H/4, W/4)\n",
    "        x = self.pool3(x)    # (batch_size, m, H/8, W/8)\n",
    "\n",
    "        # Block 4\n",
    "        x = self.conv4(x)    # (batch_size, m, H/8, W/8)\n",
    "        x = self.act4(x)     # (batch_size, m, H/8, W/8)\n",
    "        x = self.pool4(x)    # (batch_size, m, H/16, W/16)\n",
    "\n",
    "        # Block 5\n",
    "        x = self.conv5(x)    # (batch_size, m, H/16, W/16)\n",
    "        x = self.act5(x)     # (batch_size, m, H/16, W/16)\n",
    "        x = self.pool5(x)    # (batch_size, m, H/32, W/32)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, m * (H/32) * (W/32))\n",
    "\n",
    "        # Dense\n",
    "        x = self.fc1(x)            # (batch_size, dense_neurons)\n",
    "        x = self.act_fc1(x)        # (batch_size, dense_neurons)\n",
    "\n",
    "        # Output: 10 classes\n",
    "        x = self.output(x)         # (batch_size, 10)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../inaturalist_data/nature_12K_extracted already exists. Skipping extraction.\n",
      "Resizing image to resize_dim[0] x resize_dim[1] ...\n",
      "Original image size: (800, 534) (Width x Height)\n",
      "Transformed image shape: torch.Size([3, 480, 480]) (C, H, W)\n",
      "Final input shape to the model: torch.Size([1, 3, 480, 480]) (Batch, C, H, W)\n",
      "\n",
      "Model output shape: torch.Size([1, 10]) (Batch, 10)\n",
      "Raw output logits: tensor([[-0.0470,  0.0263, -0.0161,  0.0578,  0.0626, -0.0428, -0.0777, -0.0319,\n",
      "         -0.0444,  0.0573]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def extract_data_if_needed(zip_path, extract_dir):\n",
    "    \"\"\"\n",
    "    Extracts the zip file into 'extract_dir' if that folder does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        print(f\"Extracting {zip_path} to {extract_dir} ...\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=extract_dir)\n",
    "\n",
    "        print(\"Extraction done.\")\n",
    "    else:\n",
    "        print(f\"Directory {extract_dir} already exists. Skipping extraction.\")\n",
    "\n",
    "def load_single_image(image_path, resize=True, resize_dim=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads a single image with PIL, applies transforms, and returns the tensor.\n",
    "    Also prints shape info for clarity.\n",
    "    :param image_path: Path to a single .jpg file\n",
    "    :return: A PyTorch tensor of shape (1, 3, 224, 224)\n",
    "    \"\"\"\n",
    "    # Only convert to Tensor; no resizing to 224x224\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()  # # from [0..255] PIL image to [0..1] float tensor, shape => (C, H, W), range [0,1]\n",
    "    ])\n",
    "\n",
    "    # If resize is True, add resizing to the transform\n",
    "    if resize:\n",
    "        print(\"Resizing image to resize_dim[0] x resize_dim[1] ...\")\n",
    "        # Basic transform: resize to resize_dim[0] x resize_dim[1], then convert to tensor\n",
    "        transform = T.Compose([\n",
    "            T.Resize(resize_dim),  # (H, W)\n",
    "            T.ToTensor()           # from [0..255] PIL image to [0..1] float tensor, shape (C,H,W)\n",
    "        ])\n",
    "\n",
    "    # Load the image\n",
    "    pil_img = Image.open(image_path).convert('RGB')\n",
    "    print(f\"Original image size: {pil_img.size} (Width x Height)\")\n",
    "\n",
    "    # Apply transforms\n",
    "    img_tensor = transform(pil_img)  # shape: (3, H, W)\n",
    "    print(f\"Transformed image shape: {img_tensor.shape} (C, H, W)\")\n",
    "\n",
    "    # Add a batch dimension => (1, 3, H, W) i.e. (batch_size, C, H, W)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    print(f\"Final input shape to the model: {img_tensor.shape} (Batch, C, H, W)\\n\")\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "def test_model_with_image(model, image_tensor):\n",
    "    \"\"\"\n",
    "    Passes a single image tensor through the model and prints output shape.\n",
    "    :param model: Instance of MyCNN\n",
    "    :param image_tensor: shape (1, 3, H, W)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    print(f\"Model output shape: {output.shape} (Batch, 10)\")\n",
    "    print(\"Raw output logits:\", output)\n",
    "\n",
    "def main():\n",
    "    # Paths (adjust if necessary)\n",
    "    DATA_ZIP_PATH = \"../inaturalist_data/nature_12K.zip\"\n",
    "    EXTRACT_DIR   = \"../inaturalist_data/nature_12K_extracted\"\n",
    "    \n",
    "    # 1) Optional: Extract ZIP if needed\n",
    "    extract_data_if_needed(DATA_ZIP_PATH, EXTRACT_DIR)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # The ZIP has top-level folder 'inaturalist_12K' inside it.\n",
    "    # After extraction, we get:\n",
    "    #   ../inaturalist_data/inaturalist_12K_extracted/\n",
    "    #       inaturalist_12K/\n",
    "    #           train/\n",
    "    #           val/\n",
    "    #           ...\n",
    "    #\n",
    "    # So the image path must include \"inaturalist_12K\".\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # 2) Pick a single image path from the extracted data\n",
    "    #    For example, one file from 'train/Plantae' \n",
    "    sample_image_path = os.path.join(\n",
    "        EXTRACT_DIR,\n",
    "        \"inaturalist_12K\",    # top-level folder from the zip\n",
    "        \"train\",\n",
    "        \"Insecta\",\n",
    "        \"0a4a6a25d2b409ed0755097ed21fdf5b.jpg\"\n",
    "    )\n",
    "    if not os.path.isfile(sample_image_path):\n",
    "        raise FileNotFoundError(f\"Sample image not found at {sample_image_path}\")\n",
    "\n",
    "    # 3) Load and transform the image\n",
    "    image_tensor = load_single_image(sample_image_path, resize=True, resize_dim=(32*15, 32*15))\n",
    "\n",
    "    # Inspect the shape to pick your image_height, image_width\n",
    "    # For example, if the printed shape is [3, 480, 640], do:\n",
    "    _, c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "    # 4) Create model instance\n",
    "    #    Example: 16 filters each conv, kernel_size=3, dense of 128\n",
    "    model = MyCNN(in_channels=3,\n",
    "                  num_filters=16,\n",
    "                  kernel_size=3,\n",
    "                  activation_fn=nn.ReLU,\n",
    "                  dense_neurons=128,\n",
    "                  image_height=h,\n",
    "                  image_width=w)\n",
    "    print(f\"Model created with input shape: (3, {h}, {w})\")\n",
    "\n",
    "    # 5) Test forward pass\n",
    "    test_model_with_image(model, image_tensor)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10708c0b",
   "metadata": {},
   "source": [
    "# Q2 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24s020\u001b[0m (\u001b[33mda24s020-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/wandb/run-20250414_171357-fdjpdkcg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/fdjpdkcg' target=\"_blank\">northern-hill-19</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/fdjpdkcg' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo/runs/fdjpdkcg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 353\u001b[0m\n\u001b[1;32m    338\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minat_sweep_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_filters\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_root\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq2_configs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_root\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    350\u001b[0m     })\n\u001b[0;32m--> 353\u001b[0m     \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq2_configs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Usage: python train_sweep.py --data_root ../inaturalist_data/nature_12K_extracted/inaturalist_12K\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 308\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Train loop\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sweep_config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 308\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device)\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# Log to W&B\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 218\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    216\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    217\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 218\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/multiprocessing/connection.py:1135\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_jax_gpu/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import math\n",
    "import yaml\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Our CNN model from Q1, extended with optional BN, dropout, filter org\n",
    "# -------------------------------------------------------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,       # base number of filters\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,\n",
    "                 image_height=224,\n",
    "                 image_width=224,\n",
    "                 filter_organization=\"same\",\n",
    "                 batch_norm=False,\n",
    "                 dropout_rate=0.0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param filter_organization: \"same\", \"double_each_layer\", \"halve_each_layer\" etc.\n",
    "        :param batch_norm: if True, add nn.BatchNorm2d after each conv\n",
    "        :param dropout_rate: if > 0, we add nn.Dropout(...) in the final dense layers\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        Act = activation_fn  # convenience\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Decide how many filters each conv layer has\n",
    "        # Example logic:  \"same\" => all layers have the same # (num_filters).\n",
    "        #                 \"double_each_layer\" => [m, 2m, 4m, 8m, 16m]\n",
    "        #                 \"halve_each_layer\" => [m, m/2, m/4, m/8, m/16] etc\n",
    "        filter_sizes = []\n",
    "        if filter_organization == \"same\":\n",
    "            filter_sizes = [num_filters]*5\n",
    "        elif filter_organization == \"double_each_layer\":\n",
    "            filter_sizes = [num_filters*(2**i) for i in range(5)]\n",
    "        elif filter_organization == \"halve_each_layer\":\n",
    "            # integer cast for safety\n",
    "            filter_sizes = [max(1, num_filters//(2**i)) for i in range(5)]\n",
    "        else:\n",
    "            # fallback: same\n",
    "            filter_sizes = [num_filters]*5\n",
    "\n",
    "        # We'll store each conv block in a list\n",
    "        conv_layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in filter_sizes:\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_ch))\n",
    "            block.append(Act())\n",
    "            block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_layers.append(nn.Sequential(*block))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # After 5 max pools => image height/width are /32\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        final_ch = filter_sizes[-1]\n",
    "        self.flatten_dim = final_ch * reduced_height * reduced_width\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers_dense = []\n",
    "        layers_dense.append(nn.Linear(self.flatten_dim, dense_neurons))\n",
    "        if dropout_rate > 0.0:\n",
    "            layers_dense.append(nn.Dropout(dropout_rate))\n",
    "        layers_dense.append(Act())\n",
    "        layers_dense.append(nn.Linear(dense_neurons, 10))  # 10 output classes\n",
    "\n",
    "        self.fc = nn.Sequential(*layers_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through conv blocks\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Dense\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) Utility: Set random seeds for reproducibility\n",
    "# --------------------------------------------------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) Create a train/val split from iNaturalist\n",
    "# --------------------------------------------------------\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_inaturalist_train_val(data_dir,\n",
    "                               val_ratio=0.2,\n",
    "                               seed=42,\n",
    "                               augment=False,\n",
    "                               resize_dim=224):\n",
    "    \"\"\"\n",
    "    Loads the iNaturalist training data from 'data_dir' (where each subfolder \n",
    "    is a class), then splits it into train and val subsets with stratification.\n",
    "\n",
    "    :param data_dir:      Path to the folder containing subfolders of images,\n",
    "                          e.g. \".../inaturalist_12K_extracted/inaturalist_12K/train\"\n",
    "    :param val_ratio:     Fraction of data to reserve for validation (default 0.2)\n",
    "    :param seed:          Random seed to ensure reproducible splits\n",
    "    :param augment:       If True, apply creative data augmentations\n",
    "    :param resize_dim:    The final resize dimension for height & width\n",
    "                          (ideally a multiple of 32 for this CNN)\n",
    "    :return:              (train_dataset, val_dataset, class_names)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1) Define Transformations\n",
    "    # ----------------------------------------------------\n",
    "    # If you set augment=True, we'll apply some \"creative\" transformations.\n",
    "    # Otherwise, we just do a simple resize + ToTensor().\n",
    "    if augment:\n",
    "        transform_list = [\n",
    "            #  (A) Random resizing and cropping\n",
    "            T.RandomResizedCrop(size=resize_dim),\n",
    "            \n",
    "            #  (B) Random flips\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            \n",
    "            #  (C) Some random rotation\n",
    "            T.RandomRotation(degrees=30),\n",
    "            \n",
    "            #  (D) Color jitter (brightness/contrast/saturation/hue)\n",
    "            T.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2,\n",
    "                hue=0.1\n",
    "            ),\n",
    "            \n",
    "            #  (E) Small chance to invert the colors\n",
    "            T.RandomInvert(p=0.1),\n",
    "            \n",
    "            #  (F) Random perspective distortion\n",
    "            T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "            \n",
    "            #  (G) Finally, convert to Tensor\n",
    "            T.ToTensor(),\n",
    "            \n",
    "            #  (H) Optionally, random erase part of the image\n",
    "            T.RandomErasing(p=0.1)\n",
    "        ]\n",
    "    else:\n",
    "        # Minimal transform\n",
    "        transform_list = [\n",
    "            T.Resize((resize_dim, resize_dim)),\n",
    "            T.ToTensor()\n",
    "        ]\n",
    "\n",
    "    transform = T.Compose(transform_list)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2) Create the full dataset\n",
    "    # ----------------------------------------------------\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "    class_names = full_dataset.classes  # e.g. ['Amphibia', 'Animalia', ...]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3) Stratified Split\n",
    "    # ----------------------------------------------------\n",
    "    # ImageFolder automatically assigns a label per image in full_dataset.targets\n",
    "    # We can stratify on these labels to maintain class balance.\n",
    "    labels = full_dataset.targets\n",
    "    indices = list(range(len(full_dataset)))\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=val_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Build subset datasets\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "    return train_dataset, val_dataset, class_names\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4) Training loop\n",
    "# --------------------------------------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5) Main train function that W&B calls per sweep run\n",
    "# --------------------------------------------------------\n",
    "def train_and_evaluate(config):\n",
    "    sweep_config = wandb.config  # short alias\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    set_seeds(42)\n",
    "\n",
    "    # Build the model\n",
    "    if sweep_config.activation == \"relu\":\n",
    "        act_fn = nn.ReLU\n",
    "    elif sweep_config.activation == \"gelu\":\n",
    "        act_fn = nn.GELU\n",
    "    elif sweep_config.activation == \"silu\":\n",
    "        act_fn = nn.SiLU\n",
    "    elif sweep_config.activation == \"mish\":\n",
    "        act_fn = nn.Mish\n",
    "    else:\n",
    "        act_fn = nn.ReLU  # fallback\n",
    "\n",
    "    model = MyCNN(\n",
    "        num_filters=sweep_config.num_filters,\n",
    "        kernel_size=3,\n",
    "        activation_fn=act_fn,\n",
    "        dense_neurons=128,\n",
    "        image_height=config['resize_dim'],  # or whatever your images are\n",
    "        image_width=config['resize_dim'],\n",
    "        filter_organization=sweep_config.filter_organization,\n",
    "        batch_norm=sweep_config.batch_norm,\n",
    "        dropout_rate=sweep_config.dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Load data\n",
    "    train_dir = os.path.join(config['data_root'], \"train\")\n",
    "    # We'll do a random 80/20 split, but keep it consistent for the sweep\n",
    "    train_dataset, val_dataset, class_names = load_inaturalist_train_val(\n",
    "        data_dir=train_dir,\n",
    "        val_ratio=0.2,\n",
    "        seed=42,\n",
    "        augment=sweep_config.data_augmentation,\n",
    "        resize_dim=config['resize_dim']\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=sweep_config.batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=sweep_config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=sweep_config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(sweep_config.epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        # Log to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{sweep_config.epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f} | \"\n",
    "              f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "\n",
    "    # Mark run finished\n",
    "    wandb.finish()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 6) Entry point if launching script manually\n",
    "#    For sweeps, W&B will call train_and_evaluate() via agent\n",
    "# --------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to do a single run without sweeps, for debugging:\n",
    "    config = None\n",
    "    with open(\"../config/configs.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # For a single run, we can do:\n",
    "    wandb.init(project=\"inat_sweep_demo\")\n",
    "    wandb.config.update({\n",
    "        \"num_filters\": 32,\n",
    "        \"activation\": \"relu\",\n",
    "        \"filter_organization\": \"same\",\n",
    "        \"data_augmentation\": True,\n",
    "        \"batch_norm\": True,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 2,\n",
    "        \"data_root\": config['q2_configs']['data_root']\n",
    "    })\n",
    "    \n",
    "    \n",
    "    train_and_evaluate(config['q2_configs'])\n",
    "\n",
    "# Usage: python train_sweep.py --data_root ../inaturalist_data/nature_12K_extracted/inaturalist_12K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b49704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2wnmqmqb\n",
      "Sweep URL: https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4wlbx8sy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halve_each_layer\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tresize_dim: 224\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/wandb/run-20250415_211046-4wlbx8sy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/4wlbx8sy' target=\"_blank\">glamorous-sweep-1</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/4wlbx8sy' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/4wlbx8sy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] train_loss=2.3057, train_acc=0.1001, val_loss=2.3047, val_acc=0.1000\n",
      "[Epoch 2/20] train_loss=2.3032, train_acc=0.1050, val_loss=2.2980, val_acc=0.1005\n",
      "[Epoch 3/20] train_loss=2.2868, train_acc=0.1356, val_loss=2.2538, val_acc=0.1630\n",
      "[Epoch 4/20] train_loss=2.2523, train_acc=0.1585, val_loss=2.2325, val_acc=0.1725\n",
      "[Epoch 5/20] train_loss=2.2426, train_acc=0.1676, val_loss=2.2321, val_acc=0.1675\n",
      "[Epoch 6/20] train_loss=2.2404, train_acc=0.1694, val_loss=2.2257, val_acc=0.1660\n",
      "[Epoch 7/20] train_loss=2.2279, train_acc=0.1741, val_loss=2.2215, val_acc=0.1780\n",
      "[Epoch 8/20] train_loss=2.2267, train_acc=0.1763, val_loss=2.2138, val_acc=0.1685\n",
      "[Epoch 9/20] train_loss=2.2192, train_acc=0.1725, val_loss=2.2043, val_acc=0.1760\n",
      "[Epoch 10/20] train_loss=2.2084, train_acc=0.1766, val_loss=2.2040, val_acc=0.1785\n",
      "[Epoch 11/20] train_loss=2.2048, train_acc=0.1815, val_loss=2.1931, val_acc=0.1930\n",
      "[Epoch 12/20] train_loss=2.1932, train_acc=0.1880, val_loss=2.1811, val_acc=0.2000\n",
      "[Epoch 13/20] train_loss=2.1785, train_acc=0.1953, val_loss=2.1931, val_acc=0.1795\n",
      "[Epoch 14/20] train_loss=2.1818, train_acc=0.1938, val_loss=2.1807, val_acc=0.1950\n",
      "[Epoch 15/20] train_loss=2.1790, train_acc=0.1999, val_loss=2.1773, val_acc=0.1855\n",
      "[Epoch 16/20] train_loss=2.1704, train_acc=0.1970, val_loss=2.1646, val_acc=0.1965\n",
      "[Epoch 17/20] train_loss=2.1691, train_acc=0.1999, val_loss=2.1599, val_acc=0.2120\n",
      "[Epoch 18/20] train_loss=2.1562, train_acc=0.2110, val_loss=2.1525, val_acc=0.2125\n",
      "[Epoch 19/20] train_loss=2.1589, train_acc=0.2127, val_loss=2.1639, val_acc=0.2110\n",
      "[Epoch 20/20] train_loss=2.1562, train_acc=0.2115, val_loss=2.1501, val_acc=0.2170\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_acc</td><td>▁▁▃▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▅▅▅▅▆▅▆▆▇▇▆▇▆▇████</td></tr><tr><td>val_loss</td><td>██▆▅▅▄▄▄▃▃▃▂▃▂▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_acc</td><td>0.21153</td></tr><tr><td>train_loss</td><td>2.1562</td></tr><tr><td>val_accuracy</td><td>0.217</td></tr><tr><td>val_loss</td><td>2.15007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">NumFilt: 128, KSize: 5, ActFn: relu, DenseNrns: 64, FiltOrg: halve_each_layer, DataAug: True, BN: False, DR: 0.3, LR: 0.0001, BS: 64, Epochs: 20, ResizeDim: 224</strong> at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/4wlbx8sy' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/4wlbx8sy</a><br> View project at: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_211046-4wlbx8sy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p08d6eaf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tresize_dim: 352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/wandb/run-20250415_213128-p08d6eaf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/p08d6eaf' target=\"_blank\">glamorous-sweep-2</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/sweeps/2wnmqmqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/p08d6eaf' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/DA6401_Intro_to_DeepLearning_Assignment_2/runs/p08d6eaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] train_loss=2.2583, train_acc=0.1626, val_loss=2.1952, val_acc=0.2020\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run either:\n",
    "1) A single, local training run for debugging, or\n",
    "2) A Weights & Biases sweep with Bayesian optimization\n",
    "   that tries multiple hyperparameters to find the best configuration.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "# ==============================\n",
    "# 1) CNN Model Definition\n",
    "# ==============================\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,       # base number of filters\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,\n",
    "                 image_height=224,\n",
    "                 image_width=224,\n",
    "                 filter_organization=\"same\",\n",
    "                 batch_norm=False,\n",
    "                 dropout_rate=0.0\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param filter_organization: \"same\", \"double_each_layer\", \"halve_each_layer\", etc.\n",
    "        :param batch_norm: if True, adds nn.BatchNorm2d after each conv\n",
    "        :param dropout_rate: if > 0, we add nn.Dropout(...) in the final dense layers\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        self.activation_class = activation_fn\n",
    "        Act = activation_fn\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Decide how many filters each conv layer has\n",
    "        # Example logic:  \"same\" => all layers have the same # (num_filters).\n",
    "        #                 \"double_each_layer\" => [m, 2m, 4m, 8m, 16m]\n",
    "        #                 \"halve_each_layer\" => [m, m/2, m/4, m/8, m/16] etc\n",
    "        if filter_organization == \"same\":\n",
    "            filter_sizes = [num_filters]*5\n",
    "        elif filter_organization == \"double_each_layer\":\n",
    "            filter_sizes = [num_filters*(2**i) for i in range(5)]\n",
    "        elif filter_organization == \"halve_each_layer\":\n",
    "            # ensure at least 1 filter\n",
    "            filter_sizes = [max(1, num_filters//(2**i)) for i in range(5)]\n",
    "        else:\n",
    "            # fallback: same\n",
    "            filter_sizes = [num_filters]*5  # fallback\n",
    "\n",
    "        # Build convolutional blocks\n",
    "        conv_layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in filter_sizes:\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_ch))\n",
    "            block.append(Act())\n",
    "            block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_layers.append(nn.Sequential(*block))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # After 5 max pools => (height, width) / 32\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        final_ch = filter_sizes[-1]\n",
    "        self.flatten_dim = final_ch * reduced_height * reduced_width\n",
    "\n",
    "        # Fully-connected layers\n",
    "        dense_layers = []\n",
    "        dense_layers.append(nn.Linear(self.flatten_dim, dense_neurons))\n",
    "        if dropout_rate > 0.0:\n",
    "            dense_layers.append(nn.Dropout(dropout_rate))\n",
    "        dense_layers.append(Act())\n",
    "        dense_layers.append(nn.Linear(dense_neurons, 10))  # 10 output classes\n",
    "\n",
    "        self.fc = nn.Sequential(*dense_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)               # pass through 5 conv blocks\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = self.fc(x)                        # dense\n",
    "        return x\n",
    "\n",
    "# ==============================\n",
    "# 2) Utilities\n",
    "# ==============================\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_inaturalist_train_val(data_dir,\n",
    "                               val_ratio=0.2,\n",
    "                               seed=42,\n",
    "                               augment=False,\n",
    "                               resize_dim=224):\n",
    "    \"\"\"\n",
    "    Loads iNaturalist data from 'data_dir', does stratified train/val split.\n",
    "    Optionally apply data augmentations or just a simple Resize+ToTensor.\n",
    "\n",
    "    :param data_dir:      Path to the folder containing subfolders of images,\n",
    "                          e.g. \".../inaturalist_12K_extracted/inaturalist_12K/train\"\n",
    "    :param val_ratio:     Fraction of data to reserve for validation (default 0.2)\n",
    "    :param seed:          Random seed to ensure reproducible splits\n",
    "    :param augment:       If True, apply creative data augmentations\n",
    "    :param resize_dim:    The final resize dimension for height & width\n",
    "                          (ideally a multiple of 32 for this CNN)\n",
    "    :return:              (train_dataset, val_dataset, class_names)    \n",
    "    \"\"\"\n",
    "    # Define Transformations\n",
    "    # If you set augment=True, we'll apply some \"creative\" transformations.\n",
    "    # Otherwise, we just do a simple resize + ToTensor().\n",
    "    if augment:\n",
    "        transform_list = [\n",
    "            #  (A) Random resizing and cropping\n",
    "            T.RandomResizedCrop(size=resize_dim),\n",
    "            \n",
    "            #  (B) Random flips\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            \n",
    "            #  (C) Some random rotation\n",
    "            T.RandomRotation(degrees=30),\n",
    "            \n",
    "            #  (D) Color jitter (brightness/contrast/saturation/hue)\n",
    "            T.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2,\n",
    "                hue=0.1\n",
    "            ),\n",
    "            \n",
    "            #  (E) Small chance to invert the colors\n",
    "            T.RandomInvert(p=0.1),\n",
    "            \n",
    "            #  (F) Random perspective distortion\n",
    "            T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "            \n",
    "            #  (G) Finally, convert to Tensor\n",
    "            T.ToTensor(),\n",
    "            \n",
    "            #  (H) Optionally, random erase part of the image\n",
    "            T.RandomErasing(p=0.1)\n",
    "        ]\n",
    "    else:\n",
    "        # Minimal transform\n",
    "        transform_list = [\n",
    "            T.Resize((resize_dim, resize_dim)),\n",
    "            T.ToTensor()\n",
    "        ]\n",
    "\n",
    "    transform = T.Compose(transform_list)\n",
    "\n",
    "    # Full dataset\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=data_dir,\n",
    "                                                    transform=transform)\n",
    "    class_names = full_dataset.classes\n",
    "    labels = full_dataset.targets\n",
    "    indices = list(range(len(full_dataset)))\n",
    "\n",
    "    # Stratified split\n",
    "    set_seeds(seed)\n",
    "    train_indices, val_indices = train_test_split(indices,\n",
    "                                                  test_size=val_ratio,\n",
    "                                                  stratify=labels,\n",
    "                                                  random_state=seed)\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    return train_dataset, val_dataset, class_names\n",
    "\n",
    "# Training loop\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# ==============================\n",
    "# 3) Main training function\n",
    "#    (Will be used for both single run & W&B sweep)\n",
    "# ==============================\n",
    "def sweep_train():\n",
    "    \"\"\"\n",
    "    This function is called by wandb.agent(...) for each sweep run.\n",
    "    It reads the config from wandb.config, sets up data & model,\n",
    "    trains and logs results to W&B.\n",
    "    \"\"\"\n",
    "    wandb.init()  # start a W&B run\n",
    "    sweep_config = wandb.config\n",
    "    static_config = get_configs('configs.yaml')['solution_2_configs']\n",
    "\n",
    "    # Construct a run name based on various hyperparameters.\n",
    "    run_name = (\n",
    "        f\"NumFilt: {sweep_config.num_filters}, KSize: {sweep_config.kernel_size}, ActFn: {sweep_config.activation_fn}, \"\n",
    "        f\"DenseNrns: {sweep_config.dense_neurons}, FiltOrg: {sweep_config.filter_organization}, DataAug: {sweep_config.data_augmentation}, \"\n",
    "        f\"BN: {sweep_config.batch_norm}, DR: {sweep_config.dropout_rate}, LR: {sweep_config.learning_rate}, \"\n",
    "        f\"BS: {sweep_config.batch_size}, Epochs: {sweep_config.epochs}, ResizeDim: {sweep_config.resize_dim}\"\n",
    "    )\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.tags = [static_config['wandb_run_tag']]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    set_seeds(42)\n",
    "\n",
    "    # Build the model using W&B config\n",
    "    # e.g. config.activation_fn might be \"relu\"\n",
    "    act_fn = None\n",
    "    if sweep_config.activation_fn == \"relu\":\n",
    "        act_fn = nn.ReLU\n",
    "    elif sweep_config.activation_fn == \"gelu\":\n",
    "        act_fn = nn.GELU\n",
    "    elif sweep_config.activation_fn == \"silu\":\n",
    "        act_fn = nn.SiLU\n",
    "    elif sweep_config.activation_fn == \"mish\":\n",
    "        act_fn = nn.Mish\n",
    "    else:\n",
    "        act_fn = nn.ReLU # fallback\n",
    "\n",
    "    model = MyCNN(\n",
    "        in_channels=3,\n",
    "        num_filters=sweep_config.num_filters,\n",
    "        kernel_size=sweep_config.kernel_size,\n",
    "        activation_fn=act_fn,\n",
    "        dense_neurons=sweep_config.dense_neurons,\n",
    "        image_height=sweep_config.resize_dim,\n",
    "        image_width=sweep_config.resize_dim,\n",
    "        filter_organization=sweep_config.filter_organization,\n",
    "        batch_norm=sweep_config.batch_norm,\n",
    "        dropout_rate=sweep_config.dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Load data: train + val\n",
    "    train_dir = os.path.join(static_config['data_root'], \"train\")\n",
    "    train_dataset, val_dataset, class_names = load_inaturalist_train_val(\n",
    "        data_dir=train_dir,\n",
    "        val_ratio=0.2,\n",
    "        seed=42,\n",
    "        augment=sweep_config.data_augmentation,\n",
    "        resize_dim=sweep_config.resize_dim\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=sweep_config.batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=sweep_config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=sweep_config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(sweep_config.epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{sweep_config.epochs}] \"\n",
    "              f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "              f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 4) If invoked as a script\n",
    "#    - We can do a single run\n",
    "#    - Or run the W&B sweep\n",
    "# ==============================\n",
    "\n",
    "# Ensure the project root is in sys.path\n",
    "project_root = '/scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2'\n",
    "\n",
    "\n",
    "def get_configs(config_filename):\n",
    "    with open(os.path.join(project_root, \"config\", config_filename), 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load static config from YAML file\n",
    "    static_config = get_configs('configs.yaml')['solution_2_configs']\n",
    "    # Load sweep config from YAML file\n",
    "    sweep_config = get_configs('sweep_config.yaml')\n",
    "\n",
    "    # W&B sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=static_config[\"wandb_project\"])\n",
    "    wandb.agent(sweep_id, function=sweep_train, count=static_config[\"sweep_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8484daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24s020\u001b[0m (\u001b[33mda24s020-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250414_171524-ip5rwx5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2/runs/ip5rwx5d' target=\"_blank\">daily-rain-5</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2/runs/ip5rwx5d' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2/runs/ip5rwx5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | MyCNN            | 240 K  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.961     Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cede2fa80b742f7a55cac249d84a4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a559061fe5b144dc869c33ae2639c4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4123552d1fbe423fae02bc857052b7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2502bc24e554b988941d496783a8757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a1fd4848454693bd6e1676bd90dbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b92d6138fe406a86650f5278f6ce10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a327eb12960e419a95dd1acf238db92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# lightning_inat.py\n",
    "#############################################\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------------\n",
    "# 1) Set random seeds\n",
    "# --------------------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Data Module\n",
    "# --------------------------\n",
    "class InatDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for iNaturalist images. Creates stratified train/val split.\"\"\"\n",
    "    def __init__(self,\n",
    "                 data_dir: str = \"../inaturalist_data/nature_12K_extracted/inaturalist_12K/train\",\n",
    "                 val_ratio: float = 0.2,\n",
    "                 seed: int = 42,\n",
    "                 augment: bool = False,\n",
    "                 resize_dim: int = 224,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.val_ratio = val_ratio\n",
    "        self.seed = seed\n",
    "        self.augment = augment\n",
    "        self.resize_dim = resize_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Called by Lightning at the beginning (once per process in DDP).\n",
    "        set_seeds(self.seed)\n",
    "\n",
    "        # Transforms\n",
    "        if self.augment:\n",
    "            transform_list = [\n",
    "                #  (A) Random resizing and cropping\n",
    "                T.RandomResizedCrop(size=self.resize_dim),\n",
    "                \n",
    "                #  (B) Random flips\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                \n",
    "                #  (C) Some random rotation\n",
    "                T.RandomRotation(degrees=30),\n",
    "                \n",
    "                #  (D) Color jitter (brightness/contrast/saturation/hue)\n",
    "                T.ColorJitter(\n",
    "                    brightness=0.2,\n",
    "                    contrast=0.2,\n",
    "                    saturation=0.2,\n",
    "                    hue=0.1\n",
    "                ),\n",
    "                \n",
    "                #  (E) Small chance to invert the colors\n",
    "                T.RandomInvert(p=0.1),\n",
    "                \n",
    "                #  (F) Random perspective distortion\n",
    "                T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "                \n",
    "                #  (G) Finally, convert to Tensor\n",
    "                T.ToTensor(),\n",
    "                \n",
    "                #  (H) Optionally, random erase part of the image\n",
    "                T.RandomErasing(p=0.1)\n",
    "            ]\n",
    "        else:\n",
    "            transform_list = [\n",
    "                T.Resize((self.resize_dim, self.resize_dim)),\n",
    "                T.ToTensor()\n",
    "            ]\n",
    "        transform = T.Compose(transform_list)\n",
    "\n",
    "        # Full dataset\n",
    "        full_dataset = torchvision.datasets.ImageFolder(root=self.data_dir, transform=transform)\n",
    "        self.class_names = full_dataset.classes\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        # Stratified splitting\n",
    "        labels = full_dataset.targets\n",
    "        indices = np.arange(len(full_dataset))  # or just list(range(len(full_dataset)))\n",
    "\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            indices,\n",
    "            test_size=self.val_ratio,\n",
    "            random_state=self.seed,\n",
    "            stratify=labels\n",
    "        )\n",
    "        # Create subsets\n",
    "        self.train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        self.val_dataset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) The CNN model\n",
    "# --------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic CNN with 5 conv->act->pool blocks, optional BN, dropout,\n",
    "    flexible filter organization, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,\n",
    "                 image_height=224,\n",
    "                 image_width=224,\n",
    "                 filter_organization=\"same\",\n",
    "                 batch_norm=False,\n",
    "                 dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param filter_organization: \"same\", \"double_each_layer\", \"halve_each_layer\" etc.\n",
    "        :param batch_norm: if True, add nn.BatchNorm2d after each conv\n",
    "        :param dropout_rate: if > 0, we add nn.Dropout(...) in the final dense layers\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        Act = activation_fn\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Decide #filters for each layer\n",
    "        # Example logic:  \"same\" => all layers have the same # (num_filters).\n",
    "        #                 \"double_each_layer\" => [m, 2m, 4m, 8m, 16m]\n",
    "        #                 \"halve_each_layer\" => [m, m/2, m/4, m/8, m/16] etc\n",
    "        if filter_organization == \"same\":\n",
    "            filter_sizes = [num_filters]*5\n",
    "        elif filter_organization == \"double_each_layer\":\n",
    "            filter_sizes = [num_filters*(2**i) for i in range(5)]\n",
    "        elif filter_organization == \"halve_each_layer\":\n",
    "            # integer cast for safety\n",
    "            filter_sizes = [max(1, num_filters//(2**i)) for i in range(5)]\n",
    "        else:\n",
    "            # fallback: same\n",
    "            filter_sizes = [num_filters]*5\n",
    "\n",
    "        conv_layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in filter_sizes:\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_ch))\n",
    "            block.append(Act())\n",
    "            block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_layers.append(nn.Sequential(*block))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # After 5 max pools => /32\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        final_ch = filter_sizes[-1]\n",
    "        self.flatten_dim = final_ch * reduced_height * reduced_width\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers_dense = []\n",
    "        layers_dense.append(nn.Linear(self.flatten_dim, dense_neurons))\n",
    "        if dropout_rate > 0.0:\n",
    "            layers_dense.append(nn.Dropout(dropout_rate))\n",
    "        layers_dense.append(Act())\n",
    "        layers_dense.append(nn.Linear(dense_neurons, 10))  # 10 classes\n",
    "        self.fc = nn.Sequential(*layers_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through conv blocks\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Dense\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4) LightningModule\n",
    "# --------------------------\n",
    "class LitInatModel(pl.LightningModule):\n",
    "    \"\"\" Wraps MyCNN in a LightningModule for multi-GPU & logging. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels=3,\n",
    "            num_filters=32,\n",
    "            kernel_size=3,\n",
    "            activation_fn=\"relu\",\n",
    "            dense_neurons=128,\n",
    "            filter_organization=\"same\",\n",
    "            batch_norm=False,\n",
    "            dropout_rate=0.0,\n",
    "            learning_rate=1e-3,\n",
    "            batch_size=32,\n",
    "            epochs=5,\n",
    "            data_augmentation=True,\n",
    "            resize_dim=224):\n",
    "        super().__init__()\n",
    "        # Save hyperparams to check them later\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Decide activation function\n",
    "        if activation_fn.lower() == \"relu\":\n",
    "            act_fn = nn.ReLU\n",
    "        elif activation_fn.lower() == \"gelu\":\n",
    "            act_fn = nn.GELU\n",
    "        elif activation_fn.lower() == \"silu\":\n",
    "            act_fn = nn.SiLU\n",
    "        elif activation_fn.lower() == \"mish\":\n",
    "            act_fn = nn.Mish\n",
    "        else:\n",
    "            act_fn = nn.ReLU\n",
    "\n",
    "        # Build CNN\n",
    "        self.model = MyCNN(\n",
    "            in_channels=in_channels,\n",
    "            num_filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation_fn=act_fn,\n",
    "            dense_neurons=dense_neurons,\n",
    "            image_height=resize_dim,\n",
    "            image_width=resize_dim,\n",
    "            filter_organization=filter_organization,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) Main: Single-run usage\n",
    "#    For multi-GPU, set devices=4, strategy='ddp'\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"../inaturalist_data/nature_12K_extracted/inaturalist_12K\",\n",
    "                        help=\"Directory with 'train' and 'test' subfolders.\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=4, help=\"Number of GPUs to use.\")\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    # 1) W&B logger\n",
    "    wandb_logger = WandbLogger(project=\"inat_sweep_demo2\", job_type=\"pytorch_lightning_demo\")\n",
    "\n",
    "    # 2) Create model\n",
    "    model = LitInatModel(\n",
    "        in_channels=3,\n",
    "        num_filters=32,\n",
    "        kernel_size=3,\n",
    "        activation_fn=\"relu\",\n",
    "        dense_neurons=128,\n",
    "        filter_organization=\"same\",\n",
    "        batch_norm=True,\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        data_augmentation=True,\n",
    "        resize_dim=224\n",
    "    )\n",
    "\n",
    "    # 3) Create data module\n",
    "    dm = InatDataModule(\n",
    "        data_dir=os.path.join(args.data_root, \"train\"),\n",
    "        val_ratio=0.2,\n",
    "        seed=42,\n",
    "        augment=model.hparams.data_augmentation,\n",
    "        resize_dim=model.hparams.resize_dim,\n",
    "        batch_size=model.hparams.batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # 4) Create the Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=args.gpus,\n",
    "        strategy=\"ddp_notebook\",           # Data Distributed Parallel across all GPUs for notebook use: \"ddp_notebook\"; for script use: \"ddp\"\n",
    "        max_epochs=model.hparams.epochs\n",
    "    )\n",
    "\n",
    "    # 5) Start Training\n",
    "    trainer.fit(model, dm)\n",
    "    # After the run, you can check W&B for logs & multi-GPU usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165b5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/lightning_fabric/connector.py:572: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24s020\u001b[0m (\u001b[33mda24s020-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250414_171700-7zag5b29</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2/runs/7zag5b29' target=\"_blank\">rose-capybara-6</a></strong> to <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2/runs/7zag5b29' target=\"_blank\">https://wandb.ai/da24s020-indian-institute-of-technology-madras/inat_sweep_demo2/runs/7zag5b29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /scratch/storage0/da24s020/kaushik/DL/DA6401_Intro_to_DeepLearning_Assignment_2/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | MyCNN            | 240 K  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.961     Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7ea961577941c8a3b2dbb6d5701d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78ea34e104a4d2d80d1fdb627f9fb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70f23dac18a48ff8a83c3423aadb0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/scratch/storage0/da24s020/miniconda3/envs/torch_jax_gpu/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d160b9b25f8448019c3fc3262c42e54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9df8bee84946a8b17f349110e632cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8137f5dac7449bc84af6219ad682b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5fd8c06394412a58a981fcfe3e435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# lightning_inat.py\n",
    "#############################################\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "# --------------------------\n",
    "# 1) Set random seeds\n",
    "# --------------------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Data Module\n",
    "# --------------------------\n",
    "class InatDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for iNaturalist images. Creates stratified train/val split.\"\"\"\n",
    "    def __init__(self,\n",
    "                 data_dir: str = \"../inaturalist_data/nature_12K_extracted/inaturalist_12K/train\",\n",
    "                 val_ratio: float = 0.2,\n",
    "                 seed: int = 42,\n",
    "                 augment: bool = False,\n",
    "                 resize_dim: int = 224,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.val_ratio = val_ratio\n",
    "        self.seed = seed\n",
    "        self.augment = augment\n",
    "        self.resize_dim = resize_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Called by Lightning at the beginning (once per process in DDP).\n",
    "        set_seeds(self.seed)\n",
    "\n",
    "        # Transforms\n",
    "        if self.augment:\n",
    "            transform_list = [\n",
    "                #  (A) Random resizing and cropping\n",
    "                T.RandomResizedCrop(size=self.resize_dim),\n",
    "                \n",
    "                #  (B) Random flips\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                \n",
    "                #  (C) Some random rotation\n",
    "                T.RandomRotation(degrees=30),\n",
    "                \n",
    "                #  (D) Color jitter (brightness/contrast/saturation/hue)\n",
    "                T.ColorJitter(\n",
    "                    brightness=0.2,\n",
    "                    contrast=0.2,\n",
    "                    saturation=0.2,\n",
    "                    hue=0.1\n",
    "                ),\n",
    "                \n",
    "                #  (E) Small chance to invert the colors\n",
    "                T.RandomInvert(p=0.1),\n",
    "                \n",
    "                #  (F) Random perspective distortion\n",
    "                T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "                \n",
    "                #  (G) Finally, convert to Tensor\n",
    "                T.ToTensor(),\n",
    "                \n",
    "                #  (H) Optionally, random erase part of the image\n",
    "                T.RandomErasing(p=0.1)\n",
    "            ]\n",
    "        else:\n",
    "            transform_list = [\n",
    "                T.Resize((self.resize_dim, self.resize_dim)),\n",
    "                T.ToTensor()\n",
    "            ]\n",
    "        transform = T.Compose(transform_list)\n",
    "\n",
    "        # Full dataset\n",
    "        full_dataset = torchvision.datasets.ImageFolder(root=self.data_dir, transform=transform)\n",
    "        self.class_names = full_dataset.classes\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        # Stratified splitting\n",
    "        # ImageFolder stores labels in `full_dataset.targets`\n",
    "        labels = full_dataset.targets\n",
    "        indices = np.arange(len(full_dataset))  # or just list(range(len(full_dataset)))\n",
    "\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            indices,\n",
    "            test_size=self.val_ratio,\n",
    "            random_state=self.seed,\n",
    "            stratify=labels\n",
    "        )\n",
    "        # Create subsets\n",
    "        self.train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        self.val_dataset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) The CNN model\n",
    "# --------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic CNN with 5 conv->act->pool blocks, optional BN, dropout,\n",
    "    flexible filter organization, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,\n",
    "                 image_height=224,\n",
    "                 image_width=224,\n",
    "                 filter_organization=\"same\",\n",
    "                 batch_norm=False,\n",
    "                 dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param filter_organization: \"same\", \"double_each_layer\", \"halve_each_layer\" etc.\n",
    "        :param batch_norm: if True, add nn.BatchNorm2d after each conv\n",
    "        :param dropout_rate: if > 0, we add nn.Dropout(...) in the final dense layers\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        Act = activation_fn\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Decide #filters for each layer\n",
    "        # Example logic:  \"same\" => all layers have the same # (num_filters).\n",
    "        #                 \"double_each_layer\" => [m, 2m, 4m, 8m, 16m]\n",
    "        #                 \"halve_each_layer\" => [m, m/2, m/4, m/8, m/16] etc\n",
    "        if filter_organization == \"same\":\n",
    "            filter_sizes = [num_filters]*5\n",
    "        elif filter_organization == \"double_each_layer\":\n",
    "            filter_sizes = [num_filters*(2**i) for i in range(5)]\n",
    "        elif filter_organization == \"halve_each_layer\":\n",
    "            # integer cast for safety\n",
    "            filter_sizes = [max(1, num_filters//(2**i)) for i in range(5)]\n",
    "        else:\n",
    "            # fallback: same\n",
    "            filter_sizes = [num_filters]*5\n",
    "\n",
    "        conv_layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in filter_sizes:\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_ch))\n",
    "            block.append(Act())\n",
    "            block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_layers.append(nn.Sequential(*block))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # After 5 max pools => /32\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        final_ch = filter_sizes[-1]\n",
    "        self.flatten_dim = final_ch * reduced_height * reduced_width\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers_dense = []\n",
    "        layers_dense.append(nn.Linear(self.flatten_dim, dense_neurons))\n",
    "        if dropout_rate > 0.0:\n",
    "            layers_dense.append(nn.Dropout(dropout_rate))\n",
    "        layers_dense.append(Act())\n",
    "        layers_dense.append(nn.Linear(dense_neurons, 10))  # 10 classes\n",
    "        self.fc = nn.Sequential(*layers_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through conv blocks\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Dense\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4) LightningModule\n",
    "# --------------------------\n",
    "class LitInatModel(pl.LightningModule):\n",
    "    \"\"\" Wraps MyCNN in a LightningModule for multi-GPU & logging. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels=3,\n",
    "            num_filters=32,\n",
    "            kernel_size=3,\n",
    "            activation_fn=\"relu\",\n",
    "            dense_neurons=128,\n",
    "            filter_organization=\"same\",\n",
    "            data_augmentation=True,\n",
    "            batch_norm=False,\n",
    "            dropout_rate=0.0,\n",
    "            learning_rate=1e-3,\n",
    "            batch_size=32,\n",
    "            epochs=5,\n",
    "            resize_dim=224):\n",
    "        super().__init__()\n",
    "        # Save hyperparams to check them later\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Decide activation function\n",
    "        if activation_fn.lower() == \"relu\":\n",
    "            act_fn = nn.ReLU\n",
    "        elif activation_fn.lower() == \"gelu\":\n",
    "            act_fn = nn.GELU\n",
    "        elif activation_fn.lower() == \"silu\":\n",
    "            act_fn = nn.SiLU\n",
    "        elif activation_fn.lower() == \"mish\":\n",
    "            act_fn = nn.Mish\n",
    "        else:\n",
    "            act_fn = nn.ReLU\n",
    "\n",
    "        # Build CNN\n",
    "        self.model = MyCNN(\n",
    "            in_channels=in_channels,\n",
    "            num_filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation_fn=act_fn,\n",
    "            dense_neurons=dense_neurons,\n",
    "            image_height=resize_dim,\n",
    "            image_width=resize_dim,\n",
    "            filter_organization=filter_organization,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) Main: Single-run usage\n",
    "#    For multi-GPU, set devices=4, strategy='ddp'\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"../inaturalist_data/nature_12K_extracted/inaturalist_12K\",\n",
    "                        help=\"Directory with 'train' and 'test' subfolders.\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=4, help=\"Number of GPUs to use.\")\n",
    "    parser.add_argument(\"--use_compile\", action='store_true', \n",
    "                        help=\"Use torch.compile (PyTorch 2.0+) if available.\")\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    # 1) W&B logger\n",
    "    wandb_logger = WandbLogger(project=\"inat_sweep_demo2\", job_type=\"pytorch_lightning_demo\")\n",
    "\n",
    "    # 2) Create model\n",
    "    model = LitInatModel(\n",
    "        in_channels=3,\n",
    "        num_filters=32,\n",
    "        kernel_size=3,\n",
    "        activation_fn=\"relu\",\n",
    "        dense_neurons=128,\n",
    "        filter_organization=\"same\",\n",
    "        data_augmentation=True,\n",
    "        batch_norm=True,\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        resize_dim=224\n",
    "    )\n",
    "\n",
    "    # 3) Create data module\n",
    "    dm = InatDataModule(\n",
    "        data_dir=os.path.join(args.data_root, \"train\"),\n",
    "        val_ratio=0.2,\n",
    "        seed=42,\n",
    "        augment=model.hparams.data_augmentation,\n",
    "        resize_dim=model.hparams.resize_dim,\n",
    "        batch_size=model.hparams.batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # 3) (Optional) Compile for PyTorch 2.0\n",
    "    # If user wants to try Torch 2.0 compile\n",
    "    if args.use_compile and hasattr(torch, \"compile\"):\n",
    "        print(\"Compiling the model with torch.compile() ...\")\n",
    "        model.model = torch.compile(model.model)  \n",
    "        # We compile the underlying model (MyCNN). \n",
    "        # Some also do entire LightningModule, but be cautious \n",
    "        # with certain dynamic behaviors in training_step.\n",
    "\n",
    "    # 5) Setup a checkpoint callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_acc\",  # or val_loss\n",
    "        mode=\"max\",\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=\"inat-{epoch:02d}-{val_acc:.2f}\"\n",
    "    )\n",
    "\n",
    "    # 4) Create the Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=args.gpus,\n",
    "        strategy=\"ddp_notebook\",           # Data Distributed Parallel across all GPUs for notebook use: \"ddp_notebook\"; for script use: \"ddp\"\n",
    "        max_epochs=model.hparams.epochs,\n",
    "        callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
    "        precision=16,  # Optional: use mixed precision\n",
    "    )\n",
    "\n",
    "    # 5) Start Training\n",
    "    trainer.fit(model, dm)\n",
    "    # After the run, you can check W&B for logs & multi-GPU usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# lightning_inat.py (modified for W&B Sweeps)\n",
    "#############################################\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "# --------------------------\n",
    "# 1) Set random seeds\n",
    "# --------------------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Data Module\n",
    "# --------------------------\n",
    "class InatDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for iNaturalist images. Creates stratified train/val split.\"\"\"\n",
    "    def __init__(self,\n",
    "                 data_dir: str = \"../inaturalist_data/nature_12K_extracted/inaturalist_12K/train\",\n",
    "                 val_ratio: float = 0.2,\n",
    "                 seed: int = 42,\n",
    "                 augment: bool = False,\n",
    "                 resize_dim: int = 224,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.val_ratio = val_ratio\n",
    "        self.seed = seed\n",
    "        self.augment = augment\n",
    "        self.resize_dim = resize_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Called by Lightning at the beginning (once per process in DDP).\n",
    "        set_seeds(self.seed)\n",
    "\n",
    "        # Transforms\n",
    "        if self.augment:\n",
    "            transform_list = [\n",
    "                #  (A) Random resizing and cropping\n",
    "                T.RandomResizedCrop(size=self.resize_dim),\n",
    "                \n",
    "                #  (B) Random flips\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                \n",
    "                #  (C) Some random rotation\n",
    "                T.RandomRotation(degrees=30),\n",
    "                \n",
    "                #  (D) Color jitter (brightness/contrast/saturation/hue)\n",
    "                T.ColorJitter(\n",
    "                    brightness=0.2,\n",
    "                    contrast=0.2,\n",
    "                    saturation=0.2,\n",
    "                    hue=0.1\n",
    "                ),\n",
    "                \n",
    "                #  (E) Small chance to invert the colors\n",
    "                T.RandomInvert(p=0.1),\n",
    "                \n",
    "                #  (F) Random perspective distortion\n",
    "                T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "                \n",
    "                #  (G) Finally, convert to Tensor\n",
    "                T.ToTensor(),\n",
    "                \n",
    "                #  (H) Optionally, random erase part of the image\n",
    "                T.RandomErasing(p=0.1)\n",
    "            ]\n",
    "        else:\n",
    "            transform_list = [\n",
    "                T.Resize((self.resize_dim, self.resize_dim)),\n",
    "                T.ToTensor()\n",
    "            ]\n",
    "        transform = T.Compose(transform_list)\n",
    "\n",
    "        # Full dataset\n",
    "        full_dataset = torchvision.datasets.ImageFolder(root=self.data_dir, transform=transform)\n",
    "        self.class_names = full_dataset.classes\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        # Stratified splitting\n",
    "        # ImageFolder stores labels in `full_dataset.targets`\n",
    "        labels = full_dataset.targets\n",
    "        indices = np.arange(len(full_dataset))  # or just list(range(len(full_dataset)))\n",
    "\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            indices,\n",
    "            test_size=self.val_ratio,\n",
    "            random_state=self.seed,\n",
    "            stratify=labels\n",
    "        )\n",
    "        # Create subsets\n",
    "        self.train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        self.val_dataset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) The CNN model\n",
    "# --------------------------\n",
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic CNN with 5 conv->act->pool blocks, optional BN, dropout,\n",
    "    flexible filter organization, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dense_neurons=128,\n",
    "                 image_height=224,\n",
    "                 image_width=224,\n",
    "                 filter_organization=\"same\",\n",
    "                 batch_norm=False,\n",
    "                 dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param filter_organization: \"same\", \"double_each_layer\", \"halve_each_layer\" etc.\n",
    "        :param batch_norm: if True, add nn.BatchNorm2d after each conv\n",
    "        :param dropout_rate: if > 0, we add nn.Dropout(...) in the final dense layers\n",
    "        \"\"\"\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        Act = activation_fn\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Decide #filters for each layer\n",
    "        # Example logic:  \"same\" => all layers have the same # (num_filters).\n",
    "        #                 \"double_each_layer\" => [m, 2m, 4m, 8m, 16m]\n",
    "        #                 \"halve_each_layer\" => [m, m/2, m/4, m/8, m/16] etc\n",
    "        if filter_organization == \"same\":\n",
    "            filter_sizes = [num_filters]*5\n",
    "        elif filter_organization == \"double_each_layer\":\n",
    "            filter_sizes = [num_filters*(2**i) for i in range(5)]\n",
    "        elif filter_organization == \"halve_each_layer\":\n",
    "            # integer cast for safety\n",
    "            filter_sizes = [max(1, num_filters//(2**i)) for i in range(5)]\n",
    "        else:\n",
    "            # fallback: same\n",
    "            filter_sizes = [num_filters]*5\n",
    "\n",
    "        conv_layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in filter_sizes:\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm2d(out_ch))\n",
    "            block.append(Act())\n",
    "            block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_layers.append(nn.Sequential(*block))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # After 5 max pools => /32\n",
    "        reduced_height = image_height // 32\n",
    "        reduced_width  = image_width // 32\n",
    "        final_ch = filter_sizes[-1]\n",
    "        self.flatten_dim = final_ch * reduced_height * reduced_width\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers_dense = []\n",
    "        layers_dense.append(nn.Linear(self.flatten_dim, dense_neurons))\n",
    "        if dropout_rate > 0.0:\n",
    "            layers_dense.append(nn.Dropout(dropout_rate))\n",
    "        layers_dense.append(Act())\n",
    "        layers_dense.append(nn.Linear(dense_neurons, 10))  # 10 classes\n",
    "        self.fc = nn.Sequential(*layers_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through conv blocks\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Dense\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# --------------------------\n",
    "# 4) LightningModule\n",
    "# --------------------------\n",
    "class LitInatModel(pl.LightningModule):\n",
    "    \"\"\" Wraps MyCNN in a LightningModule for multi-GPU & logging. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels=3,\n",
    "            num_filters=32,\n",
    "            kernel_size=3,\n",
    "            activation_fn=\"relu\",\n",
    "            dense_neurons=128,\n",
    "            filter_organization=\"same\",\n",
    "            data_augmentation=True,\n",
    "            batch_norm=False,\n",
    "            dropout_rate=0.0,\n",
    "            learning_rate=1e-3,\n",
    "            batch_size=32,\n",
    "            epochs=5,\n",
    "            resize_dim=224):\n",
    "        super().__init__()\n",
    "        # Save hyperparams to check them later\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Decide activation function\n",
    "        if activation_fn.lower() == \"relu\":\n",
    "            act_fn = nn.ReLU\n",
    "        elif activation_fn.lower() == \"gelu\":\n",
    "            act_fn = nn.GELU\n",
    "        elif activation_fn.lower() == \"silu\":\n",
    "            act_fn = nn.SiLU\n",
    "        elif activation_fn.lower() == \"mish\":\n",
    "            act_fn = nn.Mish\n",
    "        else:\n",
    "            act_fn = nn.ReLU\n",
    "\n",
    "        # Build CNN\n",
    "        self.model = MyCNN(\n",
    "            in_channels=in_channels,\n",
    "            num_filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation_fn=act_fn,\n",
    "            dense_neurons=dense_neurons,\n",
    "            image_height=resize_dim,\n",
    "            image_width=resize_dim,\n",
    "            filter_organization=filter_organization,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "# --------------------------\n",
    "# 5) Sweep Training Function\n",
    "# --------------------------\n",
    "def sweep_train():\n",
    "    \"\"\"\n",
    "    This function is called by `wandb.agent(sweep_id, function=sweep_train, ...)`\n",
    "    for each hyperparameter config. We read `wandb.config`, create data+model,\n",
    "    optionally compile, train, then finish.\n",
    "    \"\"\"\n",
    "    wandb.init()  # start a W&B run\n",
    "    config = wandb.config\n",
    "\n",
    "    # 1) Create the data module with the sweep's hyperparams\n",
    "    dm = InatDataModule(\n",
    "        data_dir = os.path.join(config.data_root, \"train\"),\n",
    "        val_ratio=0.2,\n",
    "        seed=42,\n",
    "        augment=config.data_augmentation,\n",
    "        resize_dim=config.resize_dim,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    # 2) Create the model with the sweep config\n",
    "    model = LitInatModel(\n",
    "        in_channels=3,\n",
    "        num_filters=config.num_filters,\n",
    "        kernel_size=config.kernel_size,\n",
    "        activation_fn=config.activation_fn,\n",
    "        dense_neurons=config.dense_neurons,\n",
    "        filter_organization=config.filter_organization,\n",
    "        batch_norm=config.batch_norm,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        learning_rate=config.learning_rate,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epochs,\n",
    "        data_augmentation=config.data_augmentation,\n",
    "        resize_dim=config.resize_dim\n",
    "    )\n",
    "\n",
    "    # (Optional) torch.compile with PyTorch 2.0 if use_compile == True\n",
    "    if hasattr(torch, \"compile\") and config.use_compile:\n",
    "        print(\"Compiling the model with torch.compile() ...\")\n",
    "        model.model = torch.compile(model.model)\n",
    "\n",
    "    # 3) W&B Logger\n",
    "    wandb_logger = WandbLogger(project=\"inat_sweep_python_api\")\n",
    "\n",
    "    # 4) Checkpoint callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        filename=\"inat-{epoch:02d}-{val_acc:.2f}\",\n",
    "        dirpath=\"./checkpoints_sweep\"\n",
    "    )\n",
    "    # 5) Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=4,                  # or more if you want multi-GPU per run\n",
    "        max_epochs=config.epochs,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        precision=16\n",
    "    )\n",
    "\n",
    "    # 5) Fit\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "    wandb.finish()  # end W&B run\n",
    "\n",
    "###############################################################################\n",
    "# 6) main(): Create the sweep config in Python and run it\n",
    "###############################################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function:\n",
    "      1) Defines the sweep_config as a Python dict.\n",
    "      2) Creates the sweep via `wandb.sweep(...)`.\n",
    "      3) Starts the W&B agent with `wandb.agent(...)`, calling `sweep_train()`.\n",
    "    \"\"\"\n",
    "    import yaml\n",
    "\n",
    "    # Example sweep config as a Python dict\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\n",
    "            \"name\": \"val_acc\",\n",
    "            \"goal\": \"maximize\"\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"data_root\": {\n",
    "                \"value\": \"../inaturalist_data/nature_12K_extracted/inaturalist_12K\"\n",
    "            },\n",
    "            \"num_filters\": {\n",
    "                \"values\": [16, 32, 64]\n",
    "            },\n",
    "            \"kernel_size\": {\n",
    "                \"values\": [3, 5, 7]\n",
    "            },\n",
    "            \"activation_fn\": {\n",
    "                \"values\": [\"relu\", \"gelu\", \"silu\", \"mish\"]\n",
    "            },\n",
    "            \"dense_neurons\": {\n",
    "                \"values\": [64, 128, 256]\n",
    "            },\n",
    "            \"filter_organization\": {\n",
    "                \"values\": [\"same\", \"double_each_layer\", \"halve_each_layer\"]\n",
    "            },\n",
    "            \"data_augmentation\": {\n",
    "                \"values\": [True, False]\n",
    "            },\n",
    "            \"batch_norm\": {\n",
    "                \"values\": [True, False]\n",
    "            },\n",
    "            \"dropout_rate\": {\n",
    "                \"values\": [0.0, 0.2, 0.3]\n",
    "            },\n",
    "            \"learning_rate\": {\n",
    "                \"values\": [1e-3, 5e-4, 1e-4]\n",
    "            },\n",
    "            \"batch_size\": {\n",
    "                \"values\": [8, 32, 64]\n",
    "            },\n",
    "            \"epochs\": {\n",
    "                \"values\": [10, 20, 30]\n",
    "            },\n",
    "            \"resize_dim\": {\n",
    "                \"values\": [224, 256, 480]\n",
    "            },\n",
    "            \"use_compile\": {\n",
    "                \"values\": [True, False]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the sweep in Python (no bash needed)\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"inat_sweep_python_api\")\n",
    "\n",
    "    # Launch the agent, specifying the function to call for each run\n",
    "    # 'count=5' means run 5 different random combos. Omit or adjust as needed.\n",
    "    wandb.agent(sweep_id, function=sweep_train, count=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0426e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_jax_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
